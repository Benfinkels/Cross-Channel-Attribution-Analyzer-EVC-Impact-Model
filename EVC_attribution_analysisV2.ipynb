{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Benfinkels/Cross-Channel-Attribution-Analyzer-EVC-Impact-Model/blob/main/EVC_attribution_analysisV2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EVC & \"Ghost Traffic\" Attribution Analyzer\n",
        "\n",
        "### **The Problem: The Attribution Gap**\n",
        "Standard analytics (GA4) often fail to credit video views (EVCs) because users rarely click directly from a video to the site. Instead, they view the ad and visit later via \"Direct\" or \"Organic\" search. This leaves video campaigns looking like they have low ROI, while Organic channels appear artificially inflated.\n",
        "\n",
        "### **The Solution: Competitive Signal Unmixing**\n",
        "This tool uses **Non-Negative Least Squares (NNLS)** to mathematically \"unmix\" your traffic spikes and determine which channel is actually echoing your video performance.\n",
        "\n",
        "* **Competitive Modeling:** Unlike simple correlation, this model forces traffic sources (like Direct and Organic) to \"compete\" for credit. This prevents double-counting if multiple channels spike at the same time (solving for multicollinearity).\n",
        "* **Auto-Lag Detection:** The algorithm automatically scans a range of days (0‚Äì5) to find the exact time delay between a video view and the subsequent site visit.\n",
        "* **Ghost Efficiency:** Calculates a \"Ghost Conversion Rate\" (Coefficient) to reveal exactly how many sessions typically appear on your site for every 1 reported EVC."
      ],
      "metadata": {
        "id": "a0lYz3fjdOAR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 1: Upload Data**\n",
        "Run the cell below to launch the **Interactive Data Loader**.\n",
        "\n",
        "**File Requirements:**\n",
        "The loader includes an **Auto-Parser** that accepts most standard formats (Wide, Long, or Pivot). You need two files:\n",
        "1.  **Session File (The Effect):** A GA4 export containing `Date`, `Channel` (e.g., Session source/medium), and `Volume` (e.g., Sessions).\n",
        "2.  **Target File (The Cause):** A Google Ads export containing `Date` and your trigger metric (e.g., `EVC`, `Spend`, or `Impressions`).\n",
        "\n",
        "> **Note:** The script will automatically detect and clean date headers (pivoted data) or date rows (wide data)."
      ],
      "metadata": {
        "id": "VNSyVwLcdTkn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# CELL 0: UPLOAD BUTTON\n",
        "# ==============================================================================\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "print(\" Click below to upload your GA4 and Google Ads CSV files:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "if uploaded:\n",
        "    print(f\"\\n {len(uploaded)} file(s) uploaded successfully:\")\n",
        "    for fn in uploaded.keys():\n",
        "        print(f\"   - {fn} ({os.path.getsize(fn)/1024:.1f} KB)\")\n",
        "    print(\"\\n Now run 'Step 1: Interactive Data Loader' below.\")\n",
        "else:\n",
        "    print(\"\\n No files were uploaded.\")"
      ],
      "metadata": {
        "id": "kcm75iDBub5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CELL 1: INTERACTIVE DATA LOADER (OPTIMIZED FOR STEP 2)\n",
        "# =============================================================================\n",
        "import pandas as pd\n",
        "import os\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "import numpy as np\n",
        "\n",
        "# 1. SCAN FOR CSV FILES\n",
        "csv_files = [f for f in os.listdir('.') if f.endswith('.csv')]\n",
        "csv_files.sort(key=lambda x: os.path.getmtime(x), reverse=True) # Newest first\n",
        "\n",
        "if not csv_files:\n",
        "    print(\"‚ùå No CSV files found. Please upload files to the folder icon on the left.\")\n",
        "else:\n",
        "    # 2. CREATE WIDGETS\n",
        "    style = {'description_width': 'initial'}\n",
        "    layout = widgets.Layout(width='600px')\n",
        "\n",
        "    dd_session_file = widgets.Dropdown(options=csv_files, description='üìÇ Session Data (GA4):', style=style, layout=layout)\n",
        "    dd_target_file = widgets.Dropdown(options=csv_files, description='üéØ Target Data (EVC):', style=style, layout=layout)\n",
        "\n",
        "    # Smart defaults\n",
        "    for f in csv_files:\n",
        "        if 'analytic' in f.lower() or 'session' in f.lower(): dd_session_file.value = f\n",
        "    for f in csv_files:\n",
        "        if 'evc' in f.lower() or 'ads' in f.lower() or 'target' in f.lower(): dd_target_file.value = f\n",
        "\n",
        "    btn_load = widgets.Button(description=\"Load Selected Files\", button_style='primary', icon='upload')\n",
        "    out = widgets.Output()\n",
        "\n",
        "    display(widgets.VBox([\n",
        "        widgets.HTML(\"<h3>üìÇ Select Your Files</h3>\"),\n",
        "        dd_session_file, dd_target_file, btn_load, out\n",
        "    ]))\n",
        "\n",
        "    # 3. LOADING LOGIC\n",
        "    def clean_duplicates(df):\n",
        "        return df.loc[:, ~df.columns.duplicated()]\n",
        "\n",
        "    def process_session_file(filename):\n",
        "        \"\"\"Smart parser that differentiates between types of Wide/Pivot tables.\"\"\"\n",
        "        raw_head = pd.read_csv(filename, nrows=5)\n",
        "        cols = raw_head.columns\n",
        "\n",
        "        # Detect likely columns\n",
        "        date_col = 'Date' if 'Date' in cols else next((c for c in cols if 'date' in str(c).lower() or 'day' in str(c).lower()), None)\n",
        "        sess_col = 'Sessions' if 'Sessions' in cols else next((c for c in cols if 'session' in str(c).lower() or 'user' in str(c).lower()), None)\n",
        "\n",
        "        # CHECK 1: PIVOT FORMAT (Headers are Dates)\n",
        "        try:\n",
        "            sample_headers = cols[1:10]\n",
        "            valid_dates = pd.to_datetime(sample_headers, errors='coerce').notna().sum()\n",
        "            headers_are_dates = valid_dates > (len(sample_headers) * 0.5)\n",
        "        except: headers_are_dates = False\n",
        "\n",
        "        # --- PARSING ---\n",
        "        if headers_are_dates:\n",
        "            print(f\"   ‚Ü≥ Format: Pivot Table detected (Un-pivoting...)\")\n",
        "            df = pd.read_csv(filename)\n",
        "            id_col = df.columns[0]\n",
        "            # Melt: Turn Date Headers into a 'Date' column\n",
        "            df_long = df.melt(id_vars=[id_col], var_name='Date', value_name='Sessions')\n",
        "            df_long = df_long.rename(columns={id_col: 'Session source / medium'})\n",
        "            return clean_duplicates(df_long) # <--- FIXED BUG HERE (Was returning 'df')\n",
        "\n",
        "        # CHECK 2: ALREADY LONG FORMAT\n",
        "        elif date_col and sess_col:\n",
        "            print(f\"   ‚Ü≥ Format: Standard Long (Cleaning...)\")\n",
        "            df = pd.read_csv(filename)\n",
        "            df = clean_duplicates(df)\n",
        "\n",
        "            # Normalize names\n",
        "            if date_col != 'Date': df = df.rename(columns={date_col: 'Date'})\n",
        "            if sess_col != 'Sessions': df = df.rename(columns={sess_col: 'Sessions'})\n",
        "\n",
        "            # Identify Channel Column\n",
        "            reserved = ['Date', 'Sessions']\n",
        "            chan_cols = [c for c in df.select_dtypes(include=['object']).columns if c not in reserved]\n",
        "            if chan_cols and 'Session source / medium' not in df.columns:\n",
        "                df = df.rename(columns={chan_cols[0]: 'Session source / medium'})\n",
        "            return clean_duplicates(df)\n",
        "\n",
        "        # CHECK 3: WIDE FORMAT\n",
        "        elif date_col:\n",
        "            print(f\"   ‚Ü≥ Format: Wide (Un-pivoting...)\")\n",
        "            df = pd.read_csv(filename)\n",
        "            df = clean_duplicates(df)\n",
        "            df_long = df.melt(id_vars=[date_col], var_name='Session source / medium', value_name='Sessions')\n",
        "            df_long = df_long.rename(columns={date_col: 'Date'})\n",
        "            return clean_duplicates(df_long)\n",
        "\n",
        "        else:\n",
        "            print(f\"   ‚Ü≥ Format: Unknown. Loading as-is.\")\n",
        "            return clean_duplicates(pd.read_csv(filename))\n",
        "\n",
        "    def on_load_click(b):\n",
        "        global df_sessions, df_evc\n",
        "\n",
        "        with out:\n",
        "            clear_output()\n",
        "            s_file = dd_session_file.value\n",
        "            t_file = dd_target_file.value\n",
        "            print(f\"üîÑ Loading...\")\n",
        "\n",
        "            try:\n",
        "                # 1. LOAD SESSIONS\n",
        "                df_sessions = process_session_file(s_file)\n",
        "\n",
        "                # Critical Data Type Enforcement for Step 2\n",
        "                if 'Date' in df_sessions.columns:\n",
        "                    df_sessions['Date'] = pd.to_datetime(df_sessions['Date'], errors='coerce')\n",
        "                    df_sessions = df_sessions.dropna(subset=['Date'])\n",
        "\n",
        "                if 'Sessions' in df_sessions.columns:\n",
        "                     # Remove commas (e.g. \"1,000\") and force numeric\n",
        "                     df_sessions['Sessions'] = pd.to_numeric(df_sessions['Sessions'].astype(str).str.replace(',', ''), errors='coerce').fillna(0)\n",
        "\n",
        "                print(f\"   ‚úÖ Session Data: {len(df_sessions):,} rows loaded.\")\n",
        "\n",
        "                # 2. LOAD TARGET (EVC)\n",
        "                # Scan for offset headers\n",
        "                raw_evc = pd.read_csv(t_file, header=None, nrows=10)\n",
        "                header_row_evc = 0\n",
        "                for i, row in raw_evc.iterrows():\n",
        "                     if row.astype(str).str.contains('Date|Day|EVC', case=False, regex=True).any():\n",
        "                        header_row_evc = i; break\n",
        "\n",
        "                df_evc = pd.read_csv(t_file, header=header_row_evc)\n",
        "                df_evc = clean_duplicates(df_evc)\n",
        "\n",
        "                # Normalize EVC Dates\n",
        "                date_col_evc = next((c for c in df_evc.columns if 'date' in str(c).lower() or 'day' in str(c).lower()), 'Date')\n",
        "                df_evc = df_evc.rename(columns={date_col_evc: 'Date'})\n",
        "                df_evc['Date'] = pd.to_datetime(df_evc['Date'], errors='coerce')\n",
        "\n",
        "                print(f\"   ‚úÖ Target Data:  {len(df_evc):,} rows loaded.\")\n",
        "                print(\"\\nüéâ Success! Data is ready for Step 2.\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"\\n‚ùå Error loading files: {e}\")\n",
        "                print(\"Tip: Ensure your CSVs have standard headers like 'Date' and 'Sessions'.\")\n",
        "\n",
        "    btn_load.on_click(on_load_click)"
      ],
      "metadata": {
        "id": "k9JA6gGLgLxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 2: Configure & Run Analysis**\n",
        "\n",
        "Run the cell below to open the **Competitive Attribution Dashboard**.\n",
        "\n",
        "**Configuration Guide:**\n",
        "* **Target (EVC):** Select the \"Ghost\" signal you want to explain (e.g., *Engaged-View Conversions*).\n",
        "* **Channel Name Col:** The dimension for your traffic sources (e.g., *Session source / medium*).\n",
        "* **Scan Lag Range:** The maximum delay to test.\n",
        "    * *Recommendation:* Set to **5 days**. The tool will automatically test every delay (0 to 5) and lock onto the day where the traffic pattern best matches the EVC pattern.\n",
        "\n",
        "**Interpreting the Results Table:**\n",
        "* **Attributed EVCs:** The number of conversions the model believes actually came from this channel.\n",
        "* **Conversion Rate (Ghost):** The efficiency multiplier.\n",
        "    * *Example:* **0.05** means it takes roughly **20 Sessions** from this channel to generate **1 EVC**.\n",
        "* **Share of Explained:** The percentage of the total signal \"owned\" by this channel.\n",
        "    * *Note:* Because this is a **competitive model**, channels fight for credit. If \"Direct\" and \"Organic\" spike at the same time, the model gives credit to the one that fits the curve best, preventing double-counting."
      ],
      "metadata": {
        "id": "fsP9WOTedVnM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CELL 2: EVC \"GHOST TRAFFIC\" FINDER (NNLS & AUTO-LAG)\n",
        "# =============================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "from scipy.optimize import nnls\n",
        "from sklearn.metrics import r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "current_results_df = None\n",
        "\n",
        "def find_date_column(df):\n",
        "    possible_names = ['Date', 'date', 'Clean Date', 'Day', 'day', 'Time', 'Timestamp', 'Period', 'Week']\n",
        "    for col in df.columns:\n",
        "        if col in possible_names: return col\n",
        "    for col in df.columns:\n",
        "        if 'date' in col.lower(): return col\n",
        "    return None\n",
        "\n",
        "def run_dashboard():\n",
        "    global current_results_df, df_sessions, df_evc\n",
        "\n",
        "    # --- 1. CONNECT TO LOADED DATA ---\n",
        "    if 'df_sessions' not in globals() or 'df_evc' not in globals() or df_sessions is None:\n",
        "        print(\"‚ùå Error: Data not found. Please run 'Cell 1' first.\")\n",
        "        return\n",
        "\n",
        "    # De-duplicate\n",
        "    global_df_sessions = df_sessions.loc[:, ~df_sessions.columns.duplicated()].copy()\n",
        "    global_df_analysis = df_evc.loc[:, ~df_evc.columns.duplicated()].copy()\n",
        "\n",
        "    # --- 2. WIDGET SETUP ---\n",
        "    style = {'description_width': 'initial'}\n",
        "    layout = widgets.Layout(width='600px')\n",
        "\n",
        "    all_cols_analysis = sorted(global_df_analysis.columns.tolist())\n",
        "    all_cols_sessions = sorted(global_df_sessions.columns.tolist())\n",
        "    num_cols_sessions = sorted(global_df_sessions.select_dtypes(include=[np.number]).columns.tolist())\n",
        "\n",
        "    dd_trigger = widgets.Dropdown(options=all_cols_analysis, description='Target (EVC):', style=style, layout=layout)\n",
        "    dd_channel_col = widgets.Dropdown(options=all_cols_sessions, description='Channel Name Col:', style=style, layout=layout)\n",
        "    dd_value_col = widgets.Dropdown(options=num_cols_sessions, description='Traffic Metric (Sessions):', style=style, layout=layout)\n",
        "\n",
        "    # Smart Defaults\n",
        "    if 'EVC' in all_cols_analysis: dd_trigger.value = 'EVC'\n",
        "    if 'Session source / medium' in all_cols_sessions: dd_channel_col.value = 'Session source / medium'\n",
        "    if 'Sessions' in num_cols_sessions: dd_value_col.value = 'Sessions'\n",
        "\n",
        "    # New: Max Lag Scanner\n",
        "    slider_max_lag = widgets.IntSlider(value=5, min=1, max=14, step=1, description='Scan Lag Range (Days):', style=style, layout=layout)\n",
        "\n",
        "    btn_run = widgets.Button(description=\"Run Competitive Model\", button_style='primary', icon='calculator')\n",
        "    btn_download = widgets.Button(description=\"Download Results\", button_style='success', icon='download', disabled=True)\n",
        "    out = widgets.Output()\n",
        "\n",
        "    ui = widgets.VBox([\n",
        "        widgets.HTML(\"<h3>üëª Competitive EVC Attribution (NNLS)</h3><p>Solves for multicollinearity and auto-detects time lag.</p>\"),\n",
        "        dd_trigger, dd_channel_col, dd_value_col, slider_max_lag,\n",
        "        widgets.HBox([btn_run, btn_download]), out\n",
        "    ])\n",
        "    display(ui)\n",
        "\n",
        "    # --- DOWNLOAD LOGIC ---\n",
        "    def on_download_click(b):\n",
        "        global current_results_df\n",
        "        if current_results_df is not None:\n",
        "            filename = 'EVC_Competitive_Model.csv'\n",
        "            current_results_df.to_csv(filename, index=False)\n",
        "            from google.colab import files\n",
        "            files.download(filename)\n",
        "\n",
        "    btn_download.on_click(on_download_click)\n",
        "\n",
        "    # --- ANALYSIS LOGIC ---\n",
        "    def on_run_click(b):\n",
        "        global current_results_df\n",
        "        with out:\n",
        "            clear_output()\n",
        "            btn_download.disabled = True\n",
        "\n",
        "            trigger = dd_trigger.value\n",
        "            channel_col = dd_channel_col.value\n",
        "            value_col = dd_value_col.value\n",
        "            max_lag_scan = slider_max_lag.value\n",
        "\n",
        "            print(f\"üîÑ Preparing Data & Scanning Lags (0 to {max_lag_scan} days)...\")\n",
        "\n",
        "            # 1. Date Parsing\n",
        "            df_s = global_df_sessions.copy()\n",
        "            df_a = global_df_analysis.copy()\n",
        "            date_col_s = find_date_column(df_s)\n",
        "            date_col_a = find_date_column(df_a)\n",
        "\n",
        "            if date_col_s is None or date_col_a is None:\n",
        "                print(\"‚ùå CRITICAL ERROR: Could not find 'Date' column.\")\n",
        "                return\n",
        "\n",
        "            df_s['Date'] = pd.to_datetime(df_s[date_col_s], errors='coerce')\n",
        "            df_a['Date'] = pd.to_datetime(df_a[date_col_a], errors='coerce')\n",
        "            df_a[trigger] = pd.to_numeric(df_a[trigger], errors='coerce').fillna(0)\n",
        "\n",
        "            # 2. Pivot Sessions (The Matrix X)\n",
        "            # We need a matrix where rows=Dates, Columns=Channels\n",
        "            try:\n",
        "                df_X = df_s.groupby(['Date', channel_col])[value_col].sum().unstack(fill_value=0)\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Error Pivoting: {e}\")\n",
        "                return\n",
        "\n",
        "            # Filter out tiny channels (noise reduction)\n",
        "            # Keep channels that have at least 1% of total volume or top 50\n",
        "            total_vol = df_X.sum().sum()\n",
        "            df_X = df_X.loc[:, df_X.sum() > (total_vol * 0.001)] # 0.1% threshold\n",
        "\n",
        "            # 3. Align Target (The Vector y)\n",
        "            df_y = df_a[['Date', trigger]].groupby('Date').sum()\n",
        "\n",
        "            # Align Dates\n",
        "            common_dates = df_X.index.intersection(df_y.index)\n",
        "            if len(common_dates) < 10:\n",
        "                print(\"‚ùå Not enough overlapping dates between Sessions and EVCs.\")\n",
        "                return\n",
        "\n",
        "            # 4. Loop Lags to find Best Fit\n",
        "            best_lag = 0\n",
        "            best_score = -np.inf\n",
        "            best_weights = None\n",
        "            best_X = None\n",
        "            best_y_aligned = None\n",
        "\n",
        "            # Prepare Y (Target)\n",
        "            # In regression: EVC(t) = Beta * Sessions(t - lag)\n",
        "            # So if Lag=2, Today's EVC is explained by Sessions from 2 days ago.\n",
        "\n",
        "            for lag in range(max_lag_scan + 1):\n",
        "                # Shift X (Sessions) forward by lag to align with future EVCs\n",
        "                # e.g. Session on Jan 1 (index) -> Becomes feature for Jan 3 (shifted index)\n",
        "\n",
        "                # We simply shift the TARGET (EVC) backwards to match sessions?\n",
        "                # No, standard is: align indices.\n",
        "                # If lag=2, EVC at T is matched with X at T-2.\n",
        "\n",
        "                X_aligned = df_X.shift(lag).dropna() # Sessions shifted forward\n",
        "                y_aligned = df_y.loc[X_aligned.index] # Match dates\n",
        "\n",
        "                # Clean NaNs caused by shift\n",
        "                valid_idx = X_aligned.index.intersection(y_aligned.index)\n",
        "                X_final = X_aligned.loc[valid_idx]\n",
        "                y_final = y_aligned.loc[valid_idx][trigger].values\n",
        "\n",
        "                if len(y_final) < 5: continue\n",
        "\n",
        "                # Run NNLS (Non-Negative Least Squares)\n",
        "                # Solves argmin_x || Ax - b ||_2 for x>=0\n",
        "                weights, rss = nnls(X_final.values, y_final)\n",
        "\n",
        "                # Calculate R2 manually for NNLS\n",
        "                y_pred = np.dot(X_final.values, weights)\n",
        "                score = r2_score(y_final, y_pred)\n",
        "\n",
        "                if score > best_score:\n",
        "                    best_score = score\n",
        "                    best_lag = lag\n",
        "                    best_weights = weights\n",
        "                    best_X = X_final\n",
        "                    best_y_aligned = y_final\n",
        "\n",
        "            if best_weights is None or best_score < 0:\n",
        "                print(\"‚ö†Ô∏è No significant correlation found. Check data volume.\")\n",
        "                return\n",
        "\n",
        "            # 5. Compile Results\n",
        "            print(f\"‚úÖ Best Fit Found! Lag: {best_lag} days (R¬≤: {best_score:.3f})\")\n",
        "\n",
        "            channel_names = best_X.columns\n",
        "            total_attributed_evc = 0\n",
        "\n",
        "            results = []\n",
        "            for i, channel in enumerate(channel_names):\n",
        "                coeff = best_weights[i] # EVCs per Session\n",
        "                if coeff > 0:\n",
        "                    # Total Sessions in the model window\n",
        "                    total_sessions_channel = best_X[channel].sum()\n",
        "\n",
        "                    # Attributed EVCs = Coefficient * Volume\n",
        "                    attr_evc = coeff * total_sessions_channel\n",
        "                    total_attributed_evc += attr_evc\n",
        "\n",
        "                    results.append({\n",
        "                        'Channel': channel,\n",
        "                        'Conversion Rate (Ghost)': coeff, # The \"Beta\"\n",
        "                        'Attributed EVCs': attr_evc,\n",
        "                        'Raw Sessions': total_sessions_channel\n",
        "                    })\n",
        "\n",
        "            res_df = pd.DataFrame(results).sort_values(by='Attributed EVCs', ascending=False)\n",
        "\n",
        "            # Normalize to Observed EVCs?\n",
        "            # NNLS attempts to fit the curve magnitude.\n",
        "            # If under-predicting, it means signal is missing. If over, it's noise.\n",
        "            # We usually display the Raw Model Output.\n",
        "\n",
        "            res_df['Share of Explained EVCs'] = (res_df['Attributed EVCs'] / res_df['Attributed EVCs'].sum()) * 100\n",
        "\n",
        "            current_results_df = res_df\n",
        "            btn_download.disabled = False\n",
        "\n",
        "            # 6. Display\n",
        "            print(\"\\n\" + \"=\"*80)\n",
        "            print(f\"üèÜ ATTRIBUTION RESULT (Based on {len(best_y_aligned)} days of data)\")\n",
        "            print(\"=\"*80)\n",
        "\n",
        "            styled = res_df.style.format({\n",
        "                'Conversion Rate (Ghost)': '{:.5f}',\n",
        "                'Attributed EVCs': '{:,.1f}',\n",
        "                'Raw Sessions': '{:,.0f}',\n",
        "                'Share of Explained EVCs': '{:.1f}%'\n",
        "            }).background_gradient(subset=['Attributed EVCs'], cmap='Greens')\n",
        "\n",
        "            display(styled)\n",
        "\n",
        "            # 7. Visualization: Predicted vs Actual\n",
        "            plt.figure(figsize=(12, 5))\n",
        "            y_pred_best = np.dot(best_X.values, best_weights)\n",
        "            plt.plot(best_X.index, best_y_aligned, label='Actual Google EVCs', color='black', alpha=0.6)\n",
        "            plt.plot(best_X.index, y_pred_best, label='Model Predicted (From Traffic)', color='green', linestyle='--')\n",
        "            plt.title(f'Model Fit: Actual EVCs vs Traffic Signals (Lag: {best_lag} days)')\n",
        "            plt.legend()\n",
        "            plt.grid(True, alpha=0.3)\n",
        "            plt.show()\n",
        "\n",
        "    btn_run.on_click(on_run_click)\n",
        "\n",
        "run_dashboard()"
      ],
      "metadata": {
        "id": "oCkF42V2e9K3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### **‚ö†Ô∏è Note on Methodology**\n",
        "* **Correlation vs. Causation:** While a low P-value ($<0.05$) indicates a strong statistical link, it does not prove absolute causality. Seasonality or concurrent media events can influence these numbers.\n",
        "* **Directional Signal:** Use these results as a **directional signal** to identify which organic channels are absorbing your paid video demand."
      ],
      "metadata": {
        "id": "NtvRJ0BpdYwP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 3: Validate the Model**\n",
        "Run the cell below to visualize the accuracy of your attribution.\n",
        "\n",
        "**The Charts Explained:**\n",
        "* **Reality Check (Top):** Compares the **Actual EVCs** (Black Line) vs. the **Predicted Model** (Green Dashed).\n",
        "    * *Goal:* You want these lines to move together. If the Green line spikes when the Black line spikes, the model works.\n",
        "* **Attribution Stack (Middle):** Breaks down the Green line to show you *which channels* are driving that prediction.\n",
        "    * *Use Case:* Use this to prove to stakeholders: *\"See that spike on the 15th? That wasn't random‚Äîthat was Organic Search echoing our video campaign.\"*"
      ],
      "metadata": {
        "id": "kuEZXKBO26lv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CELL 3: ATTRIBUTION VISUALIZER (ROBUST MATCHING)\n",
        "# =============================================================================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "def run_visualizer():\n",
        "    global current_results_df, df_sessions, df_evc\n",
        "\n",
        "    # --- 1. LOAD MODEL ---\n",
        "    target_file = 'EVC_Competitive_Model.csv'\n",
        "    if os.path.exists(target_file):\n",
        "        current_results_df = pd.read_csv(target_file)\n",
        "\n",
        "    if 'current_results_df' not in globals():\n",
        "        print(\"‚ùå Error: Model results not found.\")\n",
        "        return\n",
        "\n",
        "    # --- 2. EXTRACT WEIGHTS ---\n",
        "    try:\n",
        "        # Standardize Model Channels to Lowercase for better matching\n",
        "        current_results_df['Channel_Clean'] = current_results_df['Channel'].astype(str).str.lower().str.strip()\n",
        "\n",
        "        model_weights = dict(zip(current_results_df['Channel_Clean'], current_results_df['Conversion Rate (Ghost)']))\n",
        "        share_map = dict(zip(current_results_df['Channel_Clean'], current_results_df.get('Share of Explained EVCs', [100]*len(model_weights))))\n",
        "        active_channels = {k: v for k, v in model_weights.items() if v > 0}\n",
        "\n",
        "        # Keep map of Clean -> Original Name for legend\n",
        "        name_map = dict(zip(current_results_df['Channel_Clean'], current_results_df['Channel']))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error reading model: {e}\")\n",
        "        return\n",
        "\n",
        "    # --- 3. WIDGET SETUP ---\n",
        "    style = {'description_width': 'initial'}\n",
        "    layout = widgets.Layout(width='600px')\n",
        "\n",
        "    # A. Target (EVC)\n",
        "    evc_col_guess = 'EVC'\n",
        "    if 'df_evc' in globals():\n",
        "        for c in df_evc.select_dtypes(include=[np.number]).columns:\n",
        "            if 1000 < df_evc[c].sum() < 20000: evc_col_guess = c; break\n",
        "        evc_options = sorted(df_evc.select_dtypes(include=[np.number]).columns)\n",
        "    else:\n",
        "        print(\"‚ùå Error: Raw EVC data missing.\")\n",
        "        return\n",
        "\n",
        "    dd_vis_target = widgets.Dropdown(options=evc_options, value=evc_col_guess, description='Target (EVC):', style=style, layout=layout)\n",
        "\n",
        "    # B. Input (Sessions)\n",
        "    sess_col_guess = 'Sessions'\n",
        "    if 'df_sessions' in globals():\n",
        "        num_sess = df_sessions.select_dtypes(include=[np.number]).columns\n",
        "        for c in num_sess:\n",
        "            if c.lower() in ['sessions', 'users', 'clicks', 'traffic']: sess_col_guess = c; break\n",
        "        sess_options = sorted(num_sess)\n",
        "\n",
        "        # C. Channel Column (CRITICAL NEW SELECTOR)\n",
        "        cat_sess = df_sessions.select_dtypes(exclude=[np.number]).columns\n",
        "        chan_col_guess = cat_sess[0] if len(cat_sess) > 0 else None\n",
        "        for c in cat_sess:\n",
        "            if 'source' in c.lower() or 'medium' in c.lower() or 'channel' in c.lower(): chan_col_guess = c; break\n",
        "        chan_options = sorted(cat_sess)\n",
        "    else:\n",
        "        print(\"‚ùå Error: Raw Session data missing.\")\n",
        "        return\n",
        "\n",
        "    dd_vis_metric = widgets.Dropdown(options=sess_options, value=sess_col_guess, description='Metric (Sessions):', style=style, layout=layout)\n",
        "    dd_vis_channel = widgets.Dropdown(options=chan_options, value=chan_col_guess, description='Channel Name Col:', style=style, layout=layout)\n",
        "\n",
        "    # D. Other Settings\n",
        "    slider_vis_lag = widgets.IntSlider(value=0, min=0, max=14, step=1, description='Model Lag (Days):', style=style, layout=layout)\n",
        "    toggle_smooth = widgets.Checkbox(value=True, description='Smooth Data (7-Day Avg)', style=style)\n",
        "\n",
        "    btn_viz = widgets.Button(description=\"Run Analysis\", button_style='primary', icon='check')\n",
        "    out_viz = widgets.Output()\n",
        "\n",
        "    ui = widgets.VBox([\n",
        "        widgets.HTML(\"<h3>‚öñÔ∏è Attribution Visualizer (Robust Match)</h3>\"),\n",
        "        dd_vis_target, dd_vis_metric, dd_vis_channel, slider_vis_lag, toggle_smooth, btn_viz, out_viz\n",
        "    ])\n",
        "    display(ui)\n",
        "\n",
        "    def on_viz_click(b):\n",
        "        with out_viz:\n",
        "            clear_output()\n",
        "            trigger = dd_vis_target.value\n",
        "            metric = dd_vis_metric.value\n",
        "            chan_col = dd_vis_channel.value\n",
        "            lag = slider_vis_lag.value\n",
        "            smooth = toggle_smooth.value\n",
        "\n",
        "            print(f\"‚öôÔ∏è Matching '{chan_col}' to Model...\")\n",
        "\n",
        "            # --- A. PREPARE PREDICTIONS (THE STACK) ---\n",
        "            # 1. Clean & Pivot\n",
        "            # Create a lowercase version of the channel column for matching\n",
        "            df_work = df_sessions.copy()\n",
        "            df_work['Channel_Join'] = df_work[chan_col].astype(str).str.lower().str.strip()\n",
        "\n",
        "            # Pivot on the CLEAN name\n",
        "            df_pivot = df_work.pivot_table(index='Date', columns='Channel_Join', values=metric, aggfunc='sum').fillna(0)\n",
        "            df_X = df_pivot.shift(lag).dropna()\n",
        "\n",
        "            # 2. Build Stack\n",
        "            df_plot = pd.DataFrame(index=df_X.index)\n",
        "            stack_cols = []\n",
        "\n",
        "            # Find which model keys exist in the data\n",
        "            found_channels = [c for c in active_channels.keys() if c in df_X.columns]\n",
        "\n",
        "            if not found_channels:\n",
        "                print(\"‚ùå ERROR: No matching channels found!\")\n",
        "                print(f\"   Model expects: {list(active_channels.keys())[:3]}...\")\n",
        "                print(f\"   Data contains: {list(df_X.columns)[:3]}...\")\n",
        "                print(\"   üëâ Check your 'Channel Name Col' selection.\")\n",
        "                return\n",
        "\n",
        "            # Separate Major vs Minor based on share\n",
        "            major_drivers = [k for k in found_channels if share_map.get(k, 0) > 5.0]\n",
        "            minor_drivers = [k for k in found_channels if share_map.get(k, 0) <= 5.0]\n",
        "\n",
        "            for channel in major_drivers:\n",
        "                clean_name = name_map.get(channel, channel).split(' / ')[0] # Use original name for display\n",
        "                df_plot[clean_name] = df_X[channel] * model_weights[channel]\n",
        "                stack_cols.append(clean_name)\n",
        "\n",
        "            df_plot['Other Sources'] = 0\n",
        "            has_minor = False\n",
        "            for channel in minor_drivers:\n",
        "                df_plot['Other Sources'] += df_X[channel] * model_weights[channel]\n",
        "                has_minor = True\n",
        "\n",
        "            if has_minor and df_plot['Other Sources'].sum() > 0:\n",
        "                stack_cols.append('Other Sources')\n",
        "\n",
        "            if not stack_cols:\n",
        "                print(\"‚ùå Error: Stack columns empty. Check data volume.\")\n",
        "                return\n",
        "\n",
        "            # --- B. PREPARE ACTUALS (THE LINE) ---\n",
        "            df_y = df_evc.groupby('Date')[[trigger]].sum().rename(columns={trigger: 'Actual EVCs'})\n",
        "\n",
        "            # --- C. PLOT ---\n",
        "            df_final = pd.merge(df_y, df_plot, left_index=True, right_index=True, how='inner')\n",
        "\n",
        "            if smooth:\n",
        "                df_final = df_final.rolling(window=7, min_periods=1).mean()\n",
        "\n",
        "            if df_final.empty:\n",
        "                print(\"‚ùå No overlapping data dates.\")\n",
        "                return\n",
        "\n",
        "            fig, ax = plt.subplots(figsize=(14, 7))\n",
        "\n",
        "            # Use Tab20 for more colors\n",
        "            colors = sns.color_palette(\"tab20\", len(stack_cols))\n",
        "\n",
        "            try:\n",
        "                ax.stackplot(df_final.index, df_final[stack_cols].T, labels=stack_cols, colors=colors, alpha=0.85)\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Plot Error: {e}\")\n",
        "                return\n",
        "\n",
        "            sns.lineplot(data=df_final, x=df_final.index, y='Actual EVCs', ax=ax,\n",
        "                         color='black', linewidth=3, label='Actual Reported EVCs')\n",
        "\n",
        "            ax.set_title(f\"Attribution Stack: {trigger}\", fontsize=16, fontweight='bold')\n",
        "            ax.set_ylabel(\"Daily Conversions\")\n",
        "            ax.legend(loc='upper left', bbox_to_anchor=(1.02, 1), title=\"Source\")\n",
        "            ax.grid(True, alpha=0.2)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "            # Print Stat\n",
        "            total_model = df_final[stack_cols].sum().sum()\n",
        "            total_actual = df_final['Actual EVCs'].sum()\n",
        "            print(f\"‚úÖ Prediction: {total_model:,.0f} | Actual: {total_actual:,.0f} ({total_model/total_actual:.1%})\")\n",
        "\n",
        "    btn_viz.on_click(on_viz_click)\n",
        "\n",
        "run_visualizer()"
      ],
      "metadata": {
        "id": "FUbkdc8O3qmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CELL 5: GHOST ROI CALCULATOR\n",
        "# =============================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "import os\n",
        "\n",
        "def run_roi_calculator():\n",
        "    # --- 1. LOAD MODEL ---\n",
        "    target_file = 'EVC_Competitive_Model.csv'\n",
        "    if os.path.exists(target_file):\n",
        "        df_model = pd.read_csv(target_file)\n",
        "    else:\n",
        "        print(\"‚ùå Error: 'EVC_Competitive_Model.csv' not found. Run Step 2 first.\")\n",
        "        return\n",
        "\n",
        "    # --- 2. WIDGET SETUP ---\n",
        "    style = {'description_width': 'initial'}\n",
        "    layout = widgets.Layout(width='400px')\n",
        "\n",
        "    # Financial Inputs\n",
        "    txt_aov = widgets.FloatText(value=50.00, description='Average Order Value ($):', style=style, layout=layout)\n",
        "    txt_cpa = widgets.FloatText(value=15.00, description='Target CPA ($):', style=style, layout=layout)\n",
        "\n",
        "    btn_calc = widgets.Button(description=\"Calculate ROI\", button_style='success', icon='dollar-sign')\n",
        "    out_calc = widgets.Output()\n",
        "\n",
        "    ui = widgets.VBox([\n",
        "        widgets.HTML(\"<h3>üí∞ Ghost ROI Calculator</h3><p>Translate 'Attributed Conversions' into Revenue and Media Value.</p>\"),\n",
        "        txt_aov, txt_cpa, btn_calc, out_calc\n",
        "    ])\n",
        "    display(ui)\n",
        "\n",
        "    def on_calc_click(b):\n",
        "        with out_calc:\n",
        "            clear_output()\n",
        "            aov = txt_aov.value\n",
        "            cpa = txt_cpa.value\n",
        "\n",
        "            # --- CALCULATIONS ---\n",
        "            # Revenue = EVCs * AOV\n",
        "            # Media Value = EVCs * CPA (How much you would have paid to get these elsewhere)\n",
        "\n",
        "            df_roi = df_model.copy()\n",
        "            df_roi['Ghost Revenue'] = df_roi['Attributed EVCs'] * aov\n",
        "            df_roi['Media Value Created'] = df_roi['Attributed EVCs'] * cpa\n",
        "\n",
        "            # Totals\n",
        "            total_evc = df_roi['Attributed EVCs'].sum()\n",
        "            total_rev = df_roi['Ghost Revenue'].sum()\n",
        "            total_val = df_roi['Media Value Created'].sum()\n",
        "\n",
        "            # --- OUTPUT ---\n",
        "            print(\"\\n\" + \"=\"*60)\n",
        "            print(f\"üí∏ FINANCIAL IMPACT ANALYSIS\")\n",
        "            print(\"=\"*60)\n",
        "            print(f\"‚Ä¢ Total Attributed EVCs:    {total_evc:,.0f}\")\n",
        "            print(f\"‚Ä¢ Total Ghost Revenue:      ${total_rev:,.2f}  (Based on ${aov} AOV)\")\n",
        "            print(f\"‚Ä¢ Total Media Value:        ${total_val:,.2f}  (Based on ${cpa} CPA)\")\n",
        "            print(\"-\" * 60)\n",
        "            print(f\"üí° INSIGHT: Your YouTube campaigns generated ${total_rev:,.0f} in revenue\")\n",
        "            print(f\"   that was incorrectly attributed to other channels in GA4.\")\n",
        "            print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "            # Pretty Table\n",
        "            cols_to_show = ['Channel', 'Attributed EVCs', 'Ghost Revenue', 'Media Value Created']\n",
        "\n",
        "            styled = df_roi[cols_to_show].head(10).style.format({\n",
        "                'Attributed EVCs': '{:,.1f}',\n",
        "                'Ghost Revenue': '${:,.2f}',\n",
        "                'Media Value Created': '${:,.2f}'\n",
        "            }).background_gradient(subset=['Ghost Revenue'], cmap='Greens')\n",
        "\n",
        "            display(styled)\n",
        "\n",
        "    btn_calc.on_click(on_calc_click)\n",
        "\n",
        "run_roi_calculator()"
      ],
      "metadata": {
        "id": "1NCEhpWKId7k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
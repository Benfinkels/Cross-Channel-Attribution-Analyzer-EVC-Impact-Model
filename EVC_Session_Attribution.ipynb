{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Benfinkels/Cross-Channel-Attribution-Analyzer-EVC-Impact-Model/blob/main/EVC_Session_Attribution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EVC & \"Ghost Traffic\" Attribution Analyzer\n",
        "\n",
        "### **The Problem: The Attribution Gap**\n",
        "Standard analytics (GA4) often fail to credit video views (EVCs) because users rarely click directly from a video to the site. Instead, they view the ad and visit later via \"Direct\" or \"Organic\" search. This leaves video campaigns looking like they have low ROI, while Organic channels appear artificially inflated.\n",
        "\n",
        "### **The Solution: Competitive Signal Unmixing**\n",
        "This tool uses **Non-Negative Least Squares (NNLS)** to mathematically \"unmix\" your traffic spikes and determine which channel is actually echoing your video performance.\n",
        "\n",
        "* **Competitive Modeling:** Unlike simple correlation, this model forces traffic sources (like Direct and Organic) to \"compete\" for credit. This prevents double-counting if multiple channels spike at the same time (solving for multicollinearity).\n",
        "* **Auto-Lag Detection:** The algorithm automatically scans a range of days (0‚Äì5) to find the exact time delay between a video view and the subsequent site visit.\n",
        "* **Ghost Efficiency:** Calculates a \"Ghost Conversion Rate\" (Coefficient) to reveal exactly how many sessions typically appear on your site for every 1 reported EVC."
      ],
      "metadata": {
        "id": "a0lYz3fjdOAR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 0: Install Requirements\n",
        "**Required for Step 4 (Lift Analysis).**\n",
        "\n",
        "The **CausalImpact** library (used in Step 4 to measure incremental lift) is not pre-installed in Google Colab. Run this cell once to install it.\n",
        "\n",
        "* **What it does:** Installs `pycausalimpact`, a library developed by Google for causal inference using Bayesian Structural Time-Series.\n",
        "* **When to run:** Only once per session."
      ],
      "metadata": {
        "id": "VNSyVwLcdTkn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# ==============================================================================\n",
        "# CELL 0: Install CausalImpact\n",
        "# ==============================================================================\n",
        "pip install pycausalimpact"
      ],
      "metadata": {
        "id": "me_s0SDIq7U7",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 0.5: Upload Data\n",
        "Run the cell below to upload your data files from your local machine to this notebook.\n",
        "\n",
        "**You need two CSV files:**\n",
        "1.  **Session Data (The Effect):** From GA4 (e.g., *Date, Session Source, Sessions*).\n",
        "2.  **Ad Data (The Cause):** From Google Ads (e.g., *Date, Engaged-view conversions*).\n",
        "\n",
        "> **Note:** The files are deleted when the runtime recycles, so you will need to re-upload them if you restart the notebook."
      ],
      "metadata": {
        "id": "VwoRxjpur4dN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# ==============================================================================\n",
        "# CELL 0.5: UPLOAD BUTTON\n",
        "# ==============================================================================\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "print(\" Click below to upload your GA4 and Google Ads CSV files:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "if uploaded:\n",
        "    print(f\"\\n {len(uploaded)} file(s) uploaded successfully:\")\n",
        "    for fn in uploaded.keys():\n",
        "        print(f\"   - {fn} ({os.path.getsize(fn)/1024:.1f} KB)\")\n",
        "    print(\"\\n Now run 'Step 1: Interactive Data Loader' below.\")\n",
        "else:\n",
        "    print(\"\\n No files were uploaded.\")"
      ],
      "metadata": {
        "id": "kcm75iDBub5q",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 1: Prepare Data**\n",
        "Run the cell below to launch the **Interactive Data Loader**.\n",
        "\n",
        "**File Requirements:**\n",
        "The loader includes an **Auto-Parser** that accepts most standard formats (Wide, Long, or Pivot). You need two files:\n",
        "1.  **Session File (The Effect):** A GA4 export containing `Date`, `Channel` (e.g., Session source/medium), and `Volume` (e.g., Sessions).\n",
        "2.  **Target File (The Cause):** A Google Ads export containing `Date` and your trigger metric (e.g., `EVC`, `Spend`, or `Impressions`).\n",
        "\n",
        "> **Note:** The script will automatically detect and clean date headers (pivoted data) or date rows (wide data)."
      ],
      "metadata": {
        "id": "PyF2kImFrUeo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# =============================================================================\n",
        "# CELL 1: INTERACTIVE DATA LOADER (OPTIMIZED FOR STEP 2)\n",
        "# =============================================================================\n",
        "import pandas as pd\n",
        "import os\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "import numpy as np\n",
        "\n",
        "# 1. SCAN FOR CSV FILES\n",
        "csv_files = [f for f in os.listdir('.') if f.endswith('.csv')]\n",
        "csv_files.sort(key=lambda x: os.path.getmtime(x), reverse=True) # Newest first\n",
        "\n",
        "if not csv_files:\n",
        "    print(\"‚ùå No CSV files found. Please upload files to the folder icon on the left.\")\n",
        "else:\n",
        "    # 2. CREATE WIDGETS\n",
        "    style = {'description_width': 'initial'}\n",
        "    layout = widgets.Layout(width='600px')\n",
        "\n",
        "    dd_session_file = widgets.Dropdown(options=csv_files, description='üìÇ Session Data (GA4):', style=style, layout=layout)\n",
        "    dd_target_file = widgets.Dropdown(options=csv_files, description='üéØ Target Data (EVC):', style=style, layout=layout)\n",
        "\n",
        "    # Smart defaults\n",
        "    for f in csv_files:\n",
        "        if 'analytic' in f.lower() or 'session' in f.lower(): dd_session_file.value = f\n",
        "    for f in csv_files:\n",
        "        if 'evc' in f.lower() or 'ads' in f.lower() or 'target' in f.lower(): dd_target_file.value = f\n",
        "\n",
        "    btn_load = widgets.Button(description=\"Load Selected Files\", button_style='primary', icon='upload')\n",
        "    out = widgets.Output()\n",
        "\n",
        "    display(widgets.VBox([\n",
        "        widgets.HTML(\"<h3>üìÇ Select Your Files</h3>\"),\n",
        "        dd_session_file, dd_target_file, btn_load, out\n",
        "    ]))\n",
        "\n",
        "    # 3. LOADING LOGIC\n",
        "    def clean_duplicates(df):\n",
        "        return df.loc[:, ~df.columns.duplicated()]\n",
        "\n",
        "    def process_session_file(filename):\n",
        "        \"\"\"Smart parser that differentiates between types of Wide/Pivot tables.\"\"\"\n",
        "        raw_head = pd.read_csv(filename, nrows=5)\n",
        "        cols = raw_head.columns\n",
        "\n",
        "        # Detect likely columns\n",
        "        date_col = 'Date' if 'Date' in cols else next((c for c in cols if 'date' in str(c).lower() or 'day' in str(c).lower()), None)\n",
        "        sess_col = 'Sessions' if 'Sessions' in cols else next((c for c in cols if 'session' in str(c).lower() or 'user' in str(c).lower()), None)\n",
        "\n",
        "        # CHECK 1: PIVOT FORMAT (Headers are Dates)\n",
        "        try:\n",
        "            sample_headers = cols[1:10]\n",
        "            valid_dates = pd.to_datetime(sample_headers, errors='coerce').notna().sum()\n",
        "            headers_are_dates = valid_dates > (len(sample_headers) * 0.5)\n",
        "        except: headers_are_dates = False\n",
        "\n",
        "        # --- PARSING ---\n",
        "        if headers_are_dates:\n",
        "            print(f\"   ‚Ü≥ Format: Pivot Table detected (Un-pivoting...)\")\n",
        "            df = pd.read_csv(filename)\n",
        "            id_col = df.columns[0]\n",
        "            # Melt: Turn Date Headers into a 'Date' column\n",
        "            df_long = df.melt(id_vars=[id_col], var_name='Date', value_name='Sessions')\n",
        "            df_long = df_long.rename(columns={id_col: 'Session source / medium'})\n",
        "            return clean_duplicates(df_long) # <--- FIXED BUG HERE (Was returning 'df')\n",
        "\n",
        "        # CHECK 2: ALREADY LONG FORMAT\n",
        "        elif date_col and sess_col:\n",
        "            print(f\"   ‚Ü≥ Format: Standard Long (Cleaning...)\")\n",
        "            df = pd.read_csv(filename)\n",
        "            df = clean_duplicates(df)\n",
        "\n",
        "            # Normalize names\n",
        "            if date_col != 'Date': df = df.rename(columns={date_col: 'Date'})\n",
        "            if sess_col != 'Sessions': df = df.rename(columns={sess_col: 'Sessions'})\n",
        "\n",
        "            # Identify Channel Column\n",
        "            reserved = ['Date', 'Sessions']\n",
        "            chan_cols = [c for c in df.select_dtypes(include=['object']).columns if c not in reserved]\n",
        "            if chan_cols and 'Session source / medium' not in df.columns:\n",
        "                df = df.rename(columns={chan_cols[0]: 'Session source / medium'})\n",
        "            return clean_duplicates(df)\n",
        "\n",
        "        # CHECK 3: WIDE FORMAT\n",
        "        elif date_col:\n",
        "            print(f\"   ‚Ü≥ Format: Wide (Un-pivoting...)\")\n",
        "            df = pd.read_csv(filename)\n",
        "            df = clean_duplicates(df)\n",
        "            df_long = df.melt(id_vars=[date_col], var_name='Session source / medium', value_name='Sessions')\n",
        "            df_long = df_long.rename(columns={date_col: 'Date'})\n",
        "            return clean_duplicates(df_long)\n",
        "\n",
        "        else:\n",
        "            print(f\"   ‚Ü≥ Format: Unknown. Loading as-is.\")\n",
        "            return clean_duplicates(pd.read_csv(filename))\n",
        "\n",
        "    def on_load_click(b):\n",
        "        global df_sessions, df_evc\n",
        "\n",
        "        with out:\n",
        "            clear_output()\n",
        "            s_file = dd_session_file.value\n",
        "            t_file = dd_target_file.value\n",
        "            print(f\"üîÑ Loading...\")\n",
        "\n",
        "            try:\n",
        "                # 1. LOAD SESSIONS\n",
        "                df_sessions = process_session_file(s_file)\n",
        "\n",
        "                # Critical Data Type Enforcement for Step 2\n",
        "                if 'Date' in df_sessions.columns:\n",
        "                    df_sessions['Date'] = pd.to_datetime(df_sessions['Date'], errors='coerce')\n",
        "                    df_sessions = df_sessions.dropna(subset=['Date'])\n",
        "\n",
        "                if 'Sessions' in df_sessions.columns:\n",
        "                     # Remove commas (e.g. \"1,000\") and force numeric\n",
        "                     df_sessions['Sessions'] = pd.to_numeric(df_sessions['Sessions'].astype(str).str.replace(',', ''), errors='coerce').fillna(0)\n",
        "\n",
        "                print(f\"   ‚úÖ Session Data: {len(df_sessions):,} rows loaded.\")\n",
        "\n",
        "                # 2. LOAD TARGET (EVC)\n",
        "                # Scan for offset headers\n",
        "                raw_evc = pd.read_csv(t_file, header=None, nrows=10)\n",
        "                header_row_evc = 0\n",
        "                for i, row in raw_evc.iterrows():\n",
        "                     if row.astype(str).str.contains('Date|Day|EVC', case=False, regex=True).any():\n",
        "                        header_row_evc = i; break\n",
        "\n",
        "                df_evc = pd.read_csv(t_file, header=header_row_evc)\n",
        "                df_evc = clean_duplicates(df_evc)\n",
        "\n",
        "                # Normalize EVC Dates\n",
        "                date_col_evc = next((c for c in df_evc.columns if 'date' in str(c).lower() or 'day' in str(c).lower()), 'Date')\n",
        "                df_evc = df_evc.rename(columns={date_col_evc: 'Date'})\n",
        "                df_evc['Date'] = pd.to_datetime(df_evc['Date'], errors='coerce')\n",
        "\n",
        "                print(f\"   ‚úÖ Target Data:  {len(df_evc):,} rows loaded.\")\n",
        "                print(\"\\nüéâ Success! Data is ready for Step 2.\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"\\n‚ùå Error loading files: {e}\")\n",
        "                print(\"Tip: Ensure your CSVs have standard headers like 'Date' and 'Sessions'.\")\n",
        "\n",
        "    btn_load.on_click(on_load_click)"
      ],
      "metadata": {
        "id": "k9JA6gGLgLxo",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 2: Configure & Run Analysis**\n",
        "\n",
        "Run the cell below to open the **Competitive Attribution Dashboard**.\n",
        "\n",
        "**Configuration Guide:**\n",
        "* **Target (EVC):** Select the \"Ghost\" signal you want to explain (e.g., *Engaged-View Conversions*).\n",
        "* **Channel Name Col:** The dimension for your traffic sources (e.g., *Session source / medium*).\n",
        "* **Scan Lag Range:** The maximum delay to test.\n",
        "    * *Recommendation:* Set to **5 days**. The tool will automatically test every delay (0 to 5) and lock onto the day where the traffic pattern best matches the EVC pattern.\n",
        "\n",
        "**Interpreting the Results Table:**\n",
        "* **Attributed EVCs:** The number of conversions the model believes actually came from this channel.\n",
        "* **Conversion Rate (Ghost):** The efficiency multiplier.\n",
        "    * *Example:* **0.05** means it takes roughly **20 Sessions** from this channel to generate **1 EVC**.\n",
        "* **Share of Explained:** The percentage of the total signal \"owned\" by this channel.\n",
        "    * *Note:* Because this is a **competitive model**, channels fight for credit. If \"Direct\" and \"Organic\" spike at the same time, the model gives credit to the one that fits the curve best, preventing double-counting."
      ],
      "metadata": {
        "id": "fsP9WOTedVnM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CELL 2: EVC \"GHOST TRAFFIC\" FINDER (NNLS & AUTO-LAG)\n",
        "# =============================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "from scipy.optimize import nnls\n",
        "from sklearn.metrics import r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Global storage\n",
        "current_results_df = None\n",
        "nnls_results_df = None  # New: Specific storage for NNLS results\n",
        "\n",
        "def find_date_column(df):\n",
        "    possible_names = ['Date', 'date', 'Clean Date', 'Day', 'day', 'Time', 'Timestamp', 'Period', 'Week']\n",
        "    for col in df.columns:\n",
        "        if col in possible_names: return col\n",
        "    for col in df.columns:\n",
        "        if 'date' in col.lower(): return col\n",
        "    return None\n",
        "\n",
        "def run_dashboard():\n",
        "    global current_results_df, nnls_results_df, df_sessions, df_evc\n",
        "\n",
        "    # --- 1. CONNECT TO LOADED DATA ---\n",
        "    if 'df_sessions' not in globals() or 'df_evc' not in globals() or df_sessions is None:\n",
        "        print(\"‚ùå Error: Data not found. Please run 'Cell 1' first.\")\n",
        "        return\n",
        "\n",
        "    # De-duplicate\n",
        "    global_df_sessions = df_sessions.loc[:, ~df_sessions.columns.duplicated()].copy()\n",
        "    global_df_analysis = df_evc.loc[:, ~df_evc.columns.duplicated()].copy()\n",
        "\n",
        "    # --- 2. WIDGET SETUP ---\n",
        "    style = {'description_width': 'initial'}\n",
        "    layout = widgets.Layout(width='600px')\n",
        "\n",
        "    all_cols_analysis = sorted(global_df_analysis.columns.tolist())\n",
        "    all_cols_sessions = sorted(global_df_sessions.columns.tolist())\n",
        "    num_cols_sessions = sorted(global_df_sessions.select_dtypes(include=[np.number]).columns.tolist())\n",
        "\n",
        "    dd_trigger = widgets.Dropdown(options=all_cols_analysis, description='Target (EVC):', style=style, layout=layout)\n",
        "    dd_channel_col = widgets.Dropdown(options=all_cols_sessions, description='Channel Name Col:', style=style, layout=layout)\n",
        "    dd_value_col = widgets.Dropdown(options=num_cols_sessions, description='Traffic Metric (Sessions):', style=style, layout=layout)\n",
        "\n",
        "    # Smart Defaults\n",
        "    if 'EVC' in all_cols_analysis: dd_trigger.value = 'EVC'\n",
        "    if 'Session source / medium' in all_cols_sessions: dd_channel_col.value = 'Session source / medium'\n",
        "    if 'Sessions' in num_cols_sessions: dd_value_col.value = 'Sessions'\n",
        "\n",
        "    slider_max_lag = widgets.IntSlider(value=5, min=1, max=14, step=1, description='Scan Lag Range (Days):', style=style, layout=layout)\n",
        "\n",
        "    btn_run = widgets.Button(description=\"Run Competitive Model\", button_style='primary', icon='calculator')\n",
        "    btn_download = widgets.Button(description=\"Download Results\", button_style='success', icon='download', disabled=True)\n",
        "    out = widgets.Output()\n",
        "\n",
        "    ui = widgets.VBox([\n",
        "        widgets.HTML(\"<h3>üëª Competitive EVC Attribution (NNLS)</h3><p>Solves for multicollinearity and auto-detects time lag.</p>\"),\n",
        "        dd_trigger, dd_channel_col, dd_value_col, slider_max_lag,\n",
        "        widgets.HBox([btn_run, btn_download]), out\n",
        "    ])\n",
        "    display(ui)\n",
        "\n",
        "    # --- DOWNLOAD LOGIC ---\n",
        "    def on_download_click(b):\n",
        "        global current_results_df\n",
        "        if current_results_df is not None:\n",
        "            filename = 'EVC_Competitive_Model.csv'\n",
        "            current_results_df.to_csv(filename, index=False)\n",
        "            from google.colab import files\n",
        "            files.download(filename)\n",
        "\n",
        "    btn_download.on_click(on_download_click)\n",
        "\n",
        "    # --- ANALYSIS LOGIC ---\n",
        "    def on_run_click(b):\n",
        "        global current_results_df, nnls_results_df\n",
        "        with out:\n",
        "            clear_output()\n",
        "            btn_download.disabled = True\n",
        "\n",
        "            trigger = dd_trigger.value\n",
        "            channel_col = dd_channel_col.value\n",
        "            value_col = dd_value_col.value\n",
        "            max_lag_scan = slider_max_lag.value\n",
        "\n",
        "            print(f\"üîÑ Preparing Data & Scanning Lags (0 to {max_lag_scan} days)...\")\n",
        "\n",
        "            # 1. Date Parsing\n",
        "            df_s = global_df_sessions.copy()\n",
        "            df_a = global_df_analysis.copy()\n",
        "            date_col_s = find_date_column(df_s)\n",
        "            date_col_a = find_date_column(df_a)\n",
        "\n",
        "            if date_col_s is None or date_col_a is None:\n",
        "                print(\"‚ùå CRITICAL ERROR: Could not find 'Date' column.\")\n",
        "                return\n",
        "\n",
        "            df_s['Date'] = pd.to_datetime(df_s[date_col_s], errors='coerce')\n",
        "            df_a['Date'] = pd.to_datetime(df_a[date_col_a], errors='coerce')\n",
        "            df_a[trigger] = pd.to_numeric(df_a[trigger], errors='coerce').fillna(0)\n",
        "\n",
        "            # 2. Pivot Sessions\n",
        "            try:\n",
        "                df_X = df_s.groupby(['Date', channel_col])[value_col].sum().unstack(fill_value=0)\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Error Pivoting: {e}\")\n",
        "                return\n",
        "\n",
        "            # Filter out tiny channels (noise reduction)\n",
        "            total_vol = df_X.sum().sum()\n",
        "            df_X = df_X.loc[:, df_X.sum() > (total_vol * 0.001)] # 0.1% threshold\n",
        "\n",
        "            # 3. Align Target\n",
        "            df_y = df_a[['Date', trigger]].groupby('Date').sum()\n",
        "\n",
        "            # Align Dates\n",
        "            common_dates = df_X.index.intersection(df_y.index)\n",
        "            if len(common_dates) < 10:\n",
        "                print(\"‚ùå Not enough overlapping dates between Sessions and EVCs.\")\n",
        "                return\n",
        "\n",
        "            # 4. Loop Lags to find Best Fit\n",
        "            best_lag = 0\n",
        "            best_score = -np.inf\n",
        "            best_weights = None\n",
        "            best_X = None\n",
        "            best_y_aligned = None\n",
        "\n",
        "            for lag in range(max_lag_scan + 1):\n",
        "                X_aligned = df_X.shift(lag).dropna()\n",
        "                y_aligned = df_y.loc[X_aligned.index]\n",
        "\n",
        "                valid_idx = X_aligned.index.intersection(y_aligned.index)\n",
        "                X_final = X_aligned.loc[valid_idx]\n",
        "                y_final = y_aligned.loc[valid_idx][trigger].values\n",
        "\n",
        "                if len(y_final) < 5: continue\n",
        "\n",
        "                # Run NNLS\n",
        "                weights, rss = nnls(X_final.values, y_final)\n",
        "                y_pred = np.dot(X_final.values, weights)\n",
        "                score = r2_score(y_final, y_pred)\n",
        "\n",
        "                if score > best_score:\n",
        "                    best_score = score\n",
        "                    best_lag = lag\n",
        "                    best_weights = weights\n",
        "                    best_X = X_final\n",
        "                    best_y_aligned = y_final\n",
        "\n",
        "            if best_weights is None or best_score < 0:\n",
        "                print(\"‚ö†Ô∏è No significant correlation found. Check data volume.\")\n",
        "                return\n",
        "\n",
        "            # 5. Compile Results\n",
        "            print(f\"‚úÖ Best Fit Found! Lag: {best_lag} days (R¬≤: {best_score:.3f})\")\n",
        "\n",
        "            channel_names = best_X.columns\n",
        "            results = []\n",
        "            for i, channel in enumerate(channel_names):\n",
        "                coeff = best_weights[i]\n",
        "                if coeff > 0:\n",
        "                    total_sessions_channel = best_X[channel].sum()\n",
        "                    attr_evc = coeff * total_sessions_channel\n",
        "                    results.append({\n",
        "                        'Channel': channel,\n",
        "                        'Conversion Rate (Ghost)': coeff,\n",
        "                        'Attributed EVCs': attr_evc,\n",
        "                        'Raw Sessions': total_sessions_channel\n",
        "                    })\n",
        "\n",
        "            res_df = pd.DataFrame(results).sort_values(by='Attributed EVCs', ascending=False)\n",
        "            res_df['Share of Explained EVCs'] = (res_df['Attributed EVCs'] / res_df['Attributed EVCs'].sum()) * 100\n",
        "\n",
        "            # --- SAVE RESULTS ---\n",
        "            current_results_df = res_df\n",
        "            nnls_results_df = res_df.copy() # <--- SAFETY COPY SAVED HERE\n",
        "            btn_download.disabled = False\n",
        "\n",
        "            # 6. Display\n",
        "            print(\"\\n\" + \"=\"*80)\n",
        "            print(f\"üèÜ ATTRIBUTION RESULT (Based on {len(best_y_aligned)} days of data)\")\n",
        "            print(\"=\"*80)\n",
        "\n",
        "            styled = res_df.style.format({\n",
        "                'Conversion Rate (Ghost)': '{:.5f}',\n",
        "                'Attributed EVCs': '{:,.1f}',\n",
        "                'Raw Sessions': '{:,.0f}',\n",
        "                'Share of Explained EVCs': '{:.1f}%'\n",
        "            }).background_gradient(subset=['Attributed EVCs'], cmap='Greens')\n",
        "\n",
        "            display(styled)\n",
        "\n",
        "            # 7. Visualization\n",
        "            plt.figure(figsize=(12, 5))\n",
        "            y_pred_best = np.dot(best_X.values, best_weights)\n",
        "            plt.plot(best_X.index, best_y_aligned, label='Actual Google EVCs', color='black', alpha=0.6)\n",
        "            plt.plot(best_X.index, y_pred_best, label='Model Predicted (From Traffic)', color='green', linestyle='--')\n",
        "            plt.title(f'Model Fit: Actual EVCs vs Traffic Signals (Lag: {best_lag} days)')\n",
        "            plt.legend()\n",
        "            plt.grid(True, alpha=0.3)\n",
        "            plt.show()\n",
        "\n",
        "    btn_run.on_click(on_run_click)\n",
        "\n",
        "run_dashboard()"
      ],
      "metadata": {
        "id": "oCkF42V2e9K3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Method 2: Ridge Regression (The \"Cooperative\" Model)\n",
        "\n",
        "### **Why use this?**\n",
        "Standard attribution models (like NNLS) are competitive‚Äîthey force channels to fight for credit. If \"Direct Traffic\" and \"Brand Search\" spike at the same time, the model picks one winner and gives the other zero credit.\n",
        "\n",
        "**Ridge Regression** solves this by allowing **shared credit**. It recognizes that user journeys are complex and that multiple channels often work together to drive a single conversion.\n",
        "\n",
        "**Use this model when:**\n",
        "* You see \"spiky\" results in the NNLS model.\n",
        "* You suspect YouTube is creating a \"Halo Effect\" that lifts multiple channels simultaneously (e.g., Search AND Direct)."
      ],
      "metadata": {
        "id": "0fFq2b3Tj9bx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# =============================================================================\n",
        "# CELL 2.5: RIDGE REGRESSION (AUTO-LAG DETECT)\n",
        "# =============================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Global storage\n",
        "ridge_results_df = None\n",
        "\n",
        "def run_ridge_dashboard():\n",
        "    global df_sessions, df_evc\n",
        "\n",
        "    # --- 1. SETUP ---\n",
        "    if 'df_sessions' not in globals() or 'df_evc' not in globals():\n",
        "        print(\"‚ùå Error: Data not found. Run Step 1 first.\")\n",
        "        return\n",
        "\n",
        "    style = {'description_width': 'initial'}\n",
        "    layout = widgets.Layout(width='600px')\n",
        "\n",
        "    # Detect Columns\n",
        "    evc_options = sorted(df_evc.select_dtypes(include=[np.number]).columns)\n",
        "    sess_options = sorted(df_sessions.select_dtypes(include=[np.number]).columns)\n",
        "    chan_options = sorted(df_sessions.select_dtypes(exclude=[np.number]).columns)\n",
        "\n",
        "    evc_guess = next((c for c in evc_options if 'evc' in c.lower()), evc_options[0])\n",
        "    sess_guess = next((c for c in sess_options if 'session' in c.lower()), sess_options[0])\n",
        "    chan_guess = next((c for c in chan_options if 'source' in c.lower() or 'channel' in c.lower()), chan_options[0])\n",
        "\n",
        "    # Widgets\n",
        "    dd_trigger = widgets.Dropdown(options=evc_options, value=evc_guess, description='Target (EVC):', style=style, layout=layout)\n",
        "    dd_metric = widgets.Dropdown(options=sess_options, value=sess_guess, description='Input (Sessions):', style=style, layout=layout)\n",
        "    dd_channel = widgets.Dropdown(options=chan_options, value=chan_guess, description='Channel Name Col:', style=style, layout=layout)\n",
        "\n",
        "    # Settings (Note: \"Scan Range\" instead of \"Fixed Lag\")\n",
        "    slider_alpha = widgets.FloatLogSlider(value=1.0, base=10, min=-2, max=2, step=0.1, description='Sharing Strength (Alpha):', style=style, layout=layout)\n",
        "    slider_max_lag = widgets.IntSlider(value=5, min=0, max=14, step=1, description='Scan Lag Range (Days):', style=style, layout=layout)\n",
        "\n",
        "    btn_run = widgets.Button(description=\"Run Ridge Auto-Lag\", button_style='info', icon='search')\n",
        "    out = widgets.Output()\n",
        "\n",
        "    ui = widgets.VBox([\n",
        "        widgets.HTML(\"<h3>ü§ù Ridge Regression (Auto-Lag)</h3><p>Finds the optimal time delay while allowing shared credit.</p>\"),\n",
        "        dd_trigger, dd_metric, dd_channel, slider_max_lag, slider_alpha,\n",
        "        btn_run, out\n",
        "    ])\n",
        "    display(ui)\n",
        "\n",
        "    def on_run_click(b):\n",
        "        global ridge_results_df\n",
        "        with out:\n",
        "            clear_output()\n",
        "            trigger = dd_trigger.value\n",
        "            metric = dd_metric.value\n",
        "            chan_col = dd_channel.value\n",
        "            max_lag = slider_max_lag.value\n",
        "            alpha = slider_alpha.value\n",
        "\n",
        "            print(f\"üîÑ Scanning delays (0-{max_lag} days) with Ridge (Alpha={alpha})...\")\n",
        "\n",
        "            # --- A. PREPARE DATA ---\n",
        "            try:\n",
        "                # Pivot\n",
        "                df_work = df_sessions.copy()\n",
        "                df_work['Clean_Key'] = df_work[chan_col].astype(str).str.lower().str.replace(' ', '')\n",
        "                df_X_raw = df_work.pivot_table(index='Date', columns='Clean_Key', values=metric, aggfunc='sum').fillna(0)\n",
        "\n",
        "                # Filter Noise (Top 50 or >0.1% vol)\n",
        "                df_X_raw = df_X_raw.loc[:, df_X_raw.sum() > df_X_raw.sum().sum() * 0.001]\n",
        "\n",
        "                # Target\n",
        "                df_y_raw = df_evc.groupby('Date')[[trigger]].sum()\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Error Preparing Data: {e}\")\n",
        "                return\n",
        "\n",
        "            # --- B. LOOP LAGS (THE \"BAKED IN\" PART) ---\n",
        "            best_score = -np.inf\n",
        "            best_lag = 0\n",
        "            best_model = None\n",
        "            best_X = None\n",
        "            best_y = None\n",
        "\n",
        "            for lag in range(max_lag + 1):\n",
        "                # Shift & Align\n",
        "                X_shifted = df_X_raw.shift(lag).dropna()\n",
        "                y_aligned = df_y_raw.loc[X_shifted.index]\n",
        "\n",
        "                # Intersect Dates\n",
        "                common_idx = X_shifted.index.intersection(y_aligned.index)\n",
        "                if len(common_idx) < 10: continue\n",
        "\n",
        "                X_final = X_shifted.loc[common_idx]\n",
        "                y_final = y_aligned.loc[common_idx][trigger].values\n",
        "\n",
        "                # Fit Ridge\n",
        "                model = Ridge(alpha=alpha, positive=True, fit_intercept=False)\n",
        "                model.fit(X_final, y_final)\n",
        "\n",
        "                # Score\n",
        "                score = model.score(X_final, y_final)\n",
        "\n",
        "                if score > best_score:\n",
        "                    best_score = score\n",
        "                    best_lag = lag\n",
        "                    best_model = model\n",
        "                    best_X = X_final\n",
        "                    best_y = y_final\n",
        "\n",
        "            if best_model is None:\n",
        "                print(\"‚ùå No valid correlation found.\")\n",
        "                return\n",
        "\n",
        "            print(f\"‚úÖ Best Fit Found! Lag: {best_lag} days (R¬≤: {best_score:.3f})\")\n",
        "\n",
        "            # --- C. COMPILE RESULTS ---\n",
        "            name_map = dict(zip(df_work['Clean_Key'], df_work[chan_col]))\n",
        "\n",
        "            results = []\n",
        "            y_pred = best_model.predict(best_X)\n",
        "\n",
        "            for i, col in enumerate(best_X.columns):\n",
        "                coef = best_model.coef_[i]\n",
        "                if coef > 0.000001:\n",
        "                    total_vol = df_X_raw[col].sum() # Use raw volume, not shifted\n",
        "                    attr_evc = coef * total_vol\n",
        "                    display_name = name_map.get(col, col)\n",
        "\n",
        "                    results.append({\n",
        "                        'Channel': display_name,\n",
        "                        'Conversion Rate (Ghost)': coef,\n",
        "                        'Attributed EVCs': attr_evc,\n",
        "                        'Raw Sessions': total_vol\n",
        "                    })\n",
        "\n",
        "            ridge_results_df = pd.DataFrame(results).sort_values(by='Attributed EVCs', ascending=False)\n",
        "\n",
        "            # Save for Visualizer\n",
        "            # Note: We do NOT overwrite 'current_results_df' so NNLS is preserved for comparison\n",
        "            # But we save to CSV so Step 5/6 can pick it up if desired\n",
        "            ridge_results_df.to_csv('EVC_Competitive_Model_Ridge.csv', index=False)\n",
        "\n",
        "            # Display Table\n",
        "            print(\"-\" * 60)\n",
        "            styled = ridge_results_df.head(15).style.format({\n",
        "                'Conversion Rate (Ghost)': '{:.5f}',\n",
        "                'Attributed EVCs': '{:,.1f}',\n",
        "                'Raw Sessions': '{:,.0f}'\n",
        "            }).background_gradient(subset=['Attributed EVCs'], cmap='Blues')\n",
        "            display(styled)\n",
        "\n",
        "            # --- D. PLOT ---\n",
        "            plt.figure(figsize=(12, 5))\n",
        "            plt.plot(best_X.index, best_y, label='Actual Google EVCs', color='black', alpha=0.6)\n",
        "            plt.plot(best_X.index, y_pred, label=f'Ridge Prediction (Lag {best_lag})', color='blue', linestyle='--')\n",
        "            plt.title(f'Ridge Model Fit: Shared Credit (Alpha={alpha}, Lag={best_lag})')\n",
        "            plt.legend()\n",
        "            plt.grid(True, alpha=0.3)\n",
        "            plt.show()\n",
        "\n",
        "    btn_run.on_click(on_run_click)\n",
        "\n",
        "run_ridge_dashboard()"
      ],
      "metadata": {
        "id": "8TwHtUnLiHjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### **‚ö†Ô∏è Note on Methodology**\n",
        "* **Correlation vs. Causation:** While a low P-value ($<0.05$) indicates a strong statistical link, it does not prove absolute causality. Seasonality or concurrent media events can influence these numbers.\n",
        "* **Directional Signal:** Use these results as a **directional signal** to identify which organic channels are absorbing your paid video demand."
      ],
      "metadata": {
        "id": "NtvRJ0BpdYwP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 3: Validate the Model**\n",
        "Run the cell below to visualize the accuracy of your attribution.\n",
        "\n",
        "**The Charts Explained:**\n",
        "* **Reality Check (Top):** Compares the **Actual EVCs** (Black Line) vs. the **Predicted Model** (Green Dashed).\n",
        "    * *Goal:* You want these lines to move together. If the Green line spikes when the Black line spikes, the model works.\n",
        "* **Attribution Stack (Middle):** Breaks down the Green line to show you *which channels* are driving that prediction.\n",
        "    * *Use Case:* Use this to prove to stakeholders: *\"See that spike on the 15th? That wasn't random‚Äîthat was Organic Search echoing our video campaign.\"*"
      ],
      "metadata": {
        "id": "kuEZXKBO26lv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CELL 3: DUAL-MODEL ATTRIBUTION VISUALIZER\n",
        "# =============================================================================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "def run_dual_visualizer():\n",
        "    global df_sessions, df_evc\n",
        "\n",
        "    # --- 1. SETUP ---\n",
        "    if 'df_sessions' not in globals() or 'df_evc' not in globals():\n",
        "        print(\"‚ùå Error: Raw data not found. Run Step 1 first.\")\n",
        "        return\n",
        "\n",
        "    # Check for models\n",
        "    models_found = {}\n",
        "    if os.path.exists('EVC_Competitive_Model.csv'):\n",
        "        models_found['NNLS (Competitive)'] = 'EVC_Competitive_Model.csv'\n",
        "    if os.path.exists('EVC_Competitive_Model_Ridge.csv'):\n",
        "        models_found['Ridge (Shared Credit)'] = 'EVC_Competitive_Model_Ridge.csv'\n",
        "\n",
        "    if not models_found:\n",
        "        print(\"‚ùå Error: No model files found. Run Step 2 (NNLS) and Step 2.5 (Ridge).\")\n",
        "        return\n",
        "\n",
        "    # --- 2. WIDGET SETUP ---\n",
        "    style = {'description_width': 'initial'}\n",
        "    layout = widgets.Layout(width='600px')\n",
        "\n",
        "    # Column Guessing\n",
        "    evc_options = sorted(df_evc.select_dtypes(include=[np.number]).columns)\n",
        "    sess_options = sorted(df_sessions.select_dtypes(include=[np.number]).columns)\n",
        "    chan_options = sorted(df_sessions.select_dtypes(exclude=[np.number]).columns)\n",
        "\n",
        "    evc_guess = next((c for c in evc_options if 'evc' in c.lower()), evc_options[0])\n",
        "    sess_guess = next((c for c in sess_options if 'session' in c.lower()), sess_options[0])\n",
        "    chan_guess = next((c for c in chan_options if 'source' in c.lower() or 'channel' in c.lower()), chan_options[0])\n",
        "\n",
        "    # Controls\n",
        "    dd_vis_target = widgets.Dropdown(options=evc_options, value=evc_guess, description='Target (EVC):', style=style, layout=layout)\n",
        "    dd_vis_metric = widgets.Dropdown(options=sess_options, value=sess_guess, description='Metric (Sessions):', style=style, layout=layout)\n",
        "    dd_vis_channel = widgets.Dropdown(options=chan_options, value=chan_guess, description='Channel Name Col:', style=style, layout=layout)\n",
        "\n",
        "    slider_vis_lag = widgets.IntSlider(value=1, min=0, max=14, step=1, description='Shift Stack (Days):', style=style, layout=layout)\n",
        "    toggle_smooth = widgets.Checkbox(value=False, description='Smooth Data (7-Day Avg)', style=style) # Default off to see spikes\n",
        "\n",
        "    btn_viz = widgets.Button(description=\"Visualize Comparison\", button_style='primary', icon='columns')\n",
        "    out_viz = widgets.Output()\n",
        "\n",
        "    ui = widgets.VBox([\n",
        "        widgets.HTML(\"<h3>‚öñÔ∏è Dual-Model Visualizer</h3><p>Compare the 'Spiky' Model (NNLS) vs. the 'Smooth' Model (Ridge).</p>\"),\n",
        "        dd_vis_target, dd_vis_metric, dd_vis_channel, slider_vis_lag, toggle_smooth,\n",
        "        btn_viz, out_viz\n",
        "    ])\n",
        "    display(ui)\n",
        "\n",
        "    # --- 3. HELPER TO PROCESS DATA ---\n",
        "    def prepare_model_data(model_file, trigger, metric, chan_col, lag, smooth):\n",
        "        try:\n",
        "            df_model = pd.read_csv(model_file)\n",
        "            df_model['Clean_Key'] = df_model['Channel'].astype(str).str.lower().str.replace(' ', '')\n",
        "\n",
        "            # Weights & Mapping\n",
        "            model_weights = dict(zip(df_model['Clean_Key'], df_model['Conversion Rate (Ghost)']))\n",
        "            name_map = dict(zip(df_model['Clean_Key'], df_model['Channel']))\n",
        "            active_keys = {k for k, v in model_weights.items() if v > 0}\n",
        "\n",
        "            # Pivot Session Data\n",
        "            df_work = df_sessions.copy()\n",
        "            df_work['Clean_Key'] = df_work[chan_col].astype(str).str.lower().str.replace(' ', '')\n",
        "            df_pivot = df_work.pivot_table(index='Date', columns='Clean_Key', values=metric, aggfunc='sum').fillna(0)\n",
        "\n",
        "            # Apply Lag\n",
        "            df_X = df_pivot.shift(lag).dropna()\n",
        "\n",
        "            # Build Stack\n",
        "            common_keys = [k for k in active_keys if k in df_X.columns]\n",
        "            if not common_keys: return None, None, \"No matching channels.\"\n",
        "\n",
        "            # Calculate Impact for Sorting (Top 8)\n",
        "            impact = {k: (df_X[k] * model_weights[k]).sum() for k in common_keys}\n",
        "            top_keys = sorted(common_keys, key=impact.get, reverse=True)[:8]\n",
        "\n",
        "            df_plot = pd.DataFrame(index=df_X.index)\n",
        "            stack_cols = []\n",
        "\n",
        "            for key in top_keys:\n",
        "                original_name = name_map.get(key, key)\n",
        "                df_plot[original_name] = df_X[key] * model_weights[key]\n",
        "                stack_cols.append(original_name)\n",
        "\n",
        "            # Merge Actuals\n",
        "            df_y = df_evc.groupby('Date')[[trigger]].sum().rename(columns={trigger: 'Actual EVCs'})\n",
        "            df_final = pd.merge(df_y, df_plot, left_index=True, right_index=True, how='inner')\n",
        "\n",
        "            if smooth:\n",
        "                df_final = df_final.rolling(window=7, min_periods=1).mean()\n",
        "\n",
        "            return df_final, stack_cols, None\n",
        "\n",
        "        except Exception as e:\n",
        "            return None, None, str(e)\n",
        "\n",
        "    # --- 4. PLOTTING LOGIC ---\n",
        "    def on_viz_click(b):\n",
        "        with out_viz:\n",
        "            clear_output()\n",
        "\n",
        "            # Create Subplots (1 or 2 depending on models found)\n",
        "            n_models = len(models_found)\n",
        "            fig, axes = plt.subplots(n_models, 1, figsize=(14, 6 * n_models), sharex=True)\n",
        "            if n_models == 1: axes = [axes] # Ensure iterable\n",
        "\n",
        "            params = {\n",
        "                'trigger': dd_vis_target.value,\n",
        "                'metric': dd_vis_metric.value,\n",
        "                'chan_col': dd_vis_channel.value,\n",
        "                'lag': slider_vis_lag.value,\n",
        "                'smooth': toggle_smooth.value\n",
        "            }\n",
        "\n",
        "            for i, (model_name, filename) in enumerate(models_found.items()):\n",
        "                ax = axes[i]\n",
        "                print(f\"‚öôÔ∏è Processing {model_name}...\")\n",
        "\n",
        "                df_final, stack_cols, error = prepare_model_data(filename, **params)\n",
        "\n",
        "                if error:\n",
        "                    ax.text(0.5, 0.5, f\"Error: {error}\", ha='center', transform=ax.transAxes)\n",
        "                    continue\n",
        "\n",
        "                # Plot Stack\n",
        "                colors = sns.color_palette(\"tab20\", len(stack_cols))\n",
        "                try:\n",
        "                    ax.stackplot(df_final.index, df_final[stack_cols].T, labels=stack_cols, colors=colors, alpha=0.85)\n",
        "\n",
        "                    # Plot Actual Line\n",
        "                    sns.lineplot(data=df_final, x=df_final.index, y='Actual EVCs', ax=ax,\n",
        "                                 color='black', linewidth=3, label='Actual Reported EVCs')\n",
        "\n",
        "                    # Formatting\n",
        "                    ax.set_title(f\"{model_name} Attribution\", fontsize=14, fontweight='bold')\n",
        "                    ax.set_ylabel(\"Conversions\")\n",
        "                    ax.grid(True, alpha=0.2)\n",
        "\n",
        "                    # Legend (Outside)\n",
        "                    ax.legend(loc='upper left', bbox_to_anchor=(1.01, 1), title=\"Attributed Source\")\n",
        "\n",
        "                    # Stats\n",
        "                    total_mod = df_final[stack_cols].sum().sum()\n",
        "                    total_act = df_final['Actual EVCs'].sum()\n",
        "                    ax.text(0.01, 0.95, f\"Accuracy: {total_mod/total_act:.1%}\", transform=ax.transAxes,\n",
        "                            bbox=dict(facecolor='white', alpha=0.8))\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Plotting Error on {model_name}: {e}\")\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "    btn_viz.on_click(on_viz_click)\n",
        "\n",
        "run_dual_visualizer()"
      ],
      "metadata": {
        "id": "FUbkdc8O3qmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üí∞ Step 5: Financial Impact (The \"So What?\")\n",
        "\n",
        "### **Why use this?**\n",
        "Knowing *where* traffic comes from is useful, but knowing *what it's worth* gets budget approved. This calculator translates the model's attributed \"Ghost Traffic\" into actual dollars.\n",
        "\n",
        "**Key Metrics Calculated:**\n",
        "* **Ghost Revenue:** `Attributed EVCs * Average Order Value`. The total revenue generated by YouTube that was incorrectly credited to other channels.\n",
        "* **Media Value Created:** `Attributed EVCs * Target CPA`. The efficiency value‚Äîhow much you *would* have paid to acquire these customers if you had to buy them directly on other paid channels.\n",
        "\n",
        "**Instructions:**\n",
        "1.  Enter your **Average Order Value (AOV)** (e.g., \\$50).\n",
        "2.  Enter your **Target CPA** (e.g., \\$15).\n",
        "3.  Click **Calculate ROI** to reveal the hidden financial value of your video campaign."
      ],
      "metadata": {
        "id": "gsgizci-uOKE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GHOST ROI CALCULATOR\n",
        "# =============================================================================\n",
        "# CELL 5: GHOST ROI CALCULATOR\n",
        "# =============================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "import os\n",
        "\n",
        "def run_roi_calculator():\n",
        "    # --- 1. LOAD MODEL ---\n",
        "    target_file = 'EVC_Competitive_Model.csv'\n",
        "    if os.path.exists(target_file):\n",
        "        df_model = pd.read_csv(target_file)\n",
        "    else:\n",
        "        print(\"‚ùå Error: 'EVC_Competitive_Model.csv' not found. Run Step 2 first.\")\n",
        "        return\n",
        "\n",
        "    # --- 2. WIDGET SETUP ---\n",
        "    style = {'description_width': 'initial'}\n",
        "    layout = widgets.Layout(width='400px')\n",
        "\n",
        "    # Financial Inputs\n",
        "    txt_aov = widgets.FloatText(value=50.00, description='Average Order Value ($):', style=style, layout=layout)\n",
        "    txt_cpa = widgets.FloatText(value=15.00, description='Target CPA ($):', style=style, layout=layout)\n",
        "\n",
        "    btn_calc = widgets.Button(description=\"Calculate ROI\", button_style='success', icon='dollar-sign')\n",
        "    out_calc = widgets.Output()\n",
        "\n",
        "    ui = widgets.VBox([\n",
        "        widgets.HTML(\"<h3>üí∞ Ghost ROI Calculator</h3><p>Translate 'Attributed Conversions' into Revenue and Media Value.</p>\"),\n",
        "        txt_aov, txt_cpa, btn_calc, out_calc\n",
        "    ])\n",
        "    display(ui)\n",
        "\n",
        "    def on_calc_click(b):\n",
        "        with out_calc:\n",
        "            clear_output()\n",
        "            aov = txt_aov.value\n",
        "            cpa = txt_cpa.value\n",
        "\n",
        "            # --- CALCULATIONS ---\n",
        "            # Revenue = EVCs * AOV\n",
        "            # Media Value = EVCs * CPA (How much you would have paid to get these elsewhere)\n",
        "\n",
        "            df_roi = df_model.copy()\n",
        "            df_roi['Ghost Revenue'] = df_roi['Attributed EVCs'] * aov\n",
        "            df_roi['Media Value Created'] = df_roi['Attributed EVCs'] * cpa\n",
        "\n",
        "            # Totals\n",
        "            total_evc = df_roi['Attributed EVCs'].sum()\n",
        "            total_rev = df_roi['Ghost Revenue'].sum()\n",
        "            total_val = df_roi['Media Value Created'].sum()\n",
        "\n",
        "            # --- OUTPUT ---\n",
        "            print(\"\\n\" + \"=\"*60)\n",
        "            print(f\"üí∏ FINANCIAL IMPACT ANALYSIS\")\n",
        "            print(\"=\"*60)\n",
        "            print(f\"‚Ä¢ Total Attributed EVCs:    {total_evc:,.0f}\")\n",
        "            print(f\"‚Ä¢ Total Ghost Revenue:      ${total_rev:,.2f}  (Based on ${aov} AOV)\")\n",
        "            print(f\"‚Ä¢ Total Media Value:        ${total_val:,.2f}  (Based on ${cpa} CPA)\")\n",
        "            print(\"-\" * 60)\n",
        "            print(f\"üí° INSIGHT: Your YouTube campaigns generated ${total_rev:,.0f} in revenue\")\n",
        "            print(f\"   that was incorrectly attributed to other channels in GA4.\")\n",
        "            print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "            # Pretty Table\n",
        "            cols_to_show = ['Channel', 'Attributed EVCs', 'Ghost Revenue', 'Media Value Created']\n",
        "\n",
        "            styled = df_roi[cols_to_show].head(10).style.format({\n",
        "                'Attributed EVCs': '{:,.1f}',\n",
        "                'Ghost Revenue': '${:,.2f}',\n",
        "                'Media Value Created': '${:,.2f}'\n",
        "            }).background_gradient(subset=['Ghost Revenue'], cmap='Greens')\n",
        "\n",
        "            display(styled)\n",
        "\n",
        "    btn_calc.on_click(on_calc_click)\n",
        "\n",
        "run_roi_calculator()"
      ],
      "metadata": {
        "id": "1NCEhpWKId7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# =============================================================================\n",
        "# CELL 4: CAUSAL IMPACT (WITH TOGGLE)\n",
        "# =============================================================================\n",
        "# Note: Ensure '!pip install pycausalimpact' was run in Cell 0.5\n",
        "\n",
        "import pandas as pd\n",
        "try:\n",
        "    from causalimpact import CausalImpact\n",
        "except ImportError:\n",
        "    print(\"‚ùå Error: CausalImpact not found. Please run 'Cell 0.5' to install it.\")\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "def run_causal_impact_final():\n",
        "    # --- 1. SETUP ---\n",
        "    if 'df_sessions' not in globals():\n",
        "        print(\"‚ùå Error: Session Data not found.\")\n",
        "        return\n",
        "\n",
        "    style = {'description_width': 'initial'}\n",
        "    layout = widgets.Layout(width='600px')\n",
        "\n",
        "    # Identify Columns\n",
        "    date_col = 'Date'\n",
        "    numeric_cols = sorted(df_sessions.select_dtypes(include=[np.number]).columns)\n",
        "    cat_cols = df_sessions.select_dtypes(exclude=[np.number]).columns\n",
        "    chan_col_guess = cat_cols[0] if len(cat_cols) > 0 else None\n",
        "\n",
        "    # --- 2. WIDGETS ---\n",
        "    # Mode Toggle\n",
        "    tgl_mode = widgets.ToggleButtons(\n",
        "        options=['Simple (Target Only)', 'Advanced (With Controls)'],\n",
        "        description='Analysis Mode:',\n",
        "        style={'button_width': '180px'},\n",
        "        layout=layout\n",
        "    )\n",
        "\n",
        "    # Metric & Dimension\n",
        "    dd_metric = widgets.Dropdown(options=numeric_cols, value='Sessions' if 'Sessions' in numeric_cols else numeric_cols[0], description='Metric:', layout=layout)\n",
        "    dd_chan_col = widgets.Dropdown(options=cat_cols, value=chan_col_guess, description='Channel Column:', layout=layout)\n",
        "\n",
        "    # Channel Selectors\n",
        "    dd_target_chan = widgets.Dropdown(options=[], description='üî¥ Test Channel (Ads):', layout=layout)\n",
        "    dd_controls = widgets.SelectMultiple(options=[], description='üü¢ Control Channels:', layout=layout, disabled=True)\n",
        "\n",
        "    # Dates\n",
        "    date_start = df_sessions[date_col].min()\n",
        "    picker_launch = widgets.DatePicker(description='Launch Date:', value=date_start + pd.Timedelta(days=14), layout=layout)\n",
        "\n",
        "    btn_run = widgets.Button(description=\"Calculate Lift\", button_style='danger', icon='rocket')\n",
        "    out_ci = widgets.Output()\n",
        "\n",
        "    # --- 3. INTERACTIVITY ---\n",
        "    def update_channels(*args):\n",
        "        col = dd_chan_col.value\n",
        "        if col:\n",
        "            unique_chans = sorted(df_sessions[col].astype(str).unique())\n",
        "            dd_target_chan.options = unique_chans\n",
        "            dd_controls.options = unique_chans\n",
        "\n",
        "    def on_mode_change(change):\n",
        "        # Enable/Disable Control Selector based on Toggle\n",
        "        if change['new'] == 'Simple (Target Only)':\n",
        "            dd_controls.disabled = True\n",
        "        else:\n",
        "            dd_controls.disabled = False\n",
        "\n",
        "    dd_chan_col.observe(update_channels, 'value')\n",
        "    tgl_mode.observe(on_mode_change, 'value')\n",
        "\n",
        "    # Initialize\n",
        "    update_channels()\n",
        "\n",
        "    # --- 4. DISPLAY UI ---\n",
        "    display(widgets.VBox([\n",
        "        widgets.HTML(\"<h3>üöÄ Causal Impact Analyzer</h3>\"),\n",
        "        tgl_mode,\n",
        "        picker_launch,\n",
        "        dd_metric,\n",
        "        dd_chan_col,\n",
        "        dd_target_chan,\n",
        "        dd_controls,\n",
        "        btn_run,\n",
        "        out_ci\n",
        "    ]))\n",
        "\n",
        "    # --- 5. EXECUTION LOGIC ---\n",
        "    def on_run_click(b):\n",
        "        with out_ci:\n",
        "            clear_output()\n",
        "            mode = tgl_mode.value\n",
        "            target_chan = dd_target_chan.value\n",
        "            metric = dd_metric.value\n",
        "            launch_date = pd.to_datetime(picker_launch.value)\n",
        "            chan_col_name = dd_chan_col.value\n",
        "\n",
        "            if not target_chan:\n",
        "                print(\"‚ùå Error: Please select a Test Channel.\")\n",
        "                return\n",
        "\n",
        "            print(f\"‚öôÔ∏è Preparing Data in '{mode}' mode...\")\n",
        "\n",
        "            # Pivot Data to get Daily columns per Channel\n",
        "            df_pivot = df_sessions.pivot_table(index=date_col, columns=chan_col_name, values=metric, aggfunc='sum').fillna(0)\n",
        "\n",
        "            # Select Columns based on Mode\n",
        "            if mode == 'Simple (Target Only)':\n",
        "                # Just the target (Univariate)\n",
        "                try:\n",
        "                    df_model = df_pivot[[target_chan]]\n",
        "                except KeyError:\n",
        "                    print(f\"‚ùå Error: Channel '{target_chan}' not found in data.\")\n",
        "                    return\n",
        "            else:\n",
        "                # Target + Controls (Multivariate)\n",
        "                control_chans = list(dd_controls.value)\n",
        "                if not control_chans:\n",
        "                    print(\"‚ö†Ô∏è Warning: Advanced mode selected but no controls picked. Switching to Simple mode.\")\n",
        "                    df_model = df_pivot[[target_chan]]\n",
        "                else:\n",
        "                    # Target MUST be first column for CausalImpact\n",
        "                    selected_cols = [target_chan] + control_chans\n",
        "                    df_model = df_pivot[selected_cols]\n",
        "                    print(f\"   ‚Ä¢ Using {len(control_chans)} control channels as baseline.\")\n",
        "\n",
        "            # Define Periods\n",
        "            pre_period = [str(df_model.index.min().date()), str((launch_date - pd.Timedelta(days=1)).date())]\n",
        "            post_period = [str(launch_date.date()), str(df_model.index.max().date())]\n",
        "\n",
        "            print(f\"üìä Calculating Lift on '{target_chan}'...\")\n",
        "\n",
        "            try:\n",
        "                # Run Model\n",
        "                ci = CausalImpact(df_model, pre_period, post_period)\n",
        "\n",
        "                # Text Report\n",
        "                print(\"\\n\" + \"=\"*60)\n",
        "                print(ci.summary())\n",
        "                print(\"=\"*60)\n",
        "\n",
        "                # Visual Report\n",
        "                ci.plot()\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"\\n‚ùå Calculation Error: {e}\")\n",
        "                if mode == 'Advanced (With Controls)':\n",
        "                    print(\"   üëâ Tip: Ensure your Control Channels are stable and didn't ALSO receive ad traffic.\")\n",
        "\n",
        "    btn_run.on_click(on_run_click)\n",
        "\n",
        "run_causal_impact_final()"
      ],
      "metadata": {
        "id": "wKAa5o6WqrNK",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CELL 6: ADVANCED EXECUTIVE REPORT (DUAL MODEL + CHARTS)\n",
        "# =============================================================================\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import base64\n",
        "import pandas as pd\n",
        "import io\n",
        "import numpy as np\n",
        "\n",
        "# --- HELPER: CONVERT PLOT TO BASE64 IMAGE ---\n",
        "def plot_to_base64(df_model, df_sess, df_target, title_prefix):\n",
        "    \"\"\"\n",
        "    Generates a static Stackplot for the HTML report.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # 1. Clean & Prepare Data\n",
        "        # Re-create the matching logic from Step 3 (simplified for reporting)\n",
        "        df_model['Clean_Key'] = df_model['Channel'].astype(str).str.lower().str.replace(' ', '')\n",
        "        model_weights = dict(zip(df_model['Clean_Key'], df_model['Conversion Rate (Ghost)']))\n",
        "        name_map = dict(zip(df_model['Clean_Key'], df_model['Channel']))\n",
        "        active_keys = {k for k, v in model_weights.items() if v > 0}\n",
        "\n",
        "        # Guess columns (Target & Session)\n",
        "        trigger_col = df_target.select_dtypes(include=[np.number]).columns[0]\n",
        "        sess_col = df_sess.select_dtypes(include=[np.number]).columns[0]\n",
        "        chan_col = df_sess.select_dtypes(exclude=[np.number]).columns[0]\n",
        "\n",
        "        # Pivot Sessions\n",
        "        df_work = df_sess.copy()\n",
        "        df_work['Clean_Key'] = df_work[chan_col].astype(str).str.lower().str.replace(' ', '')\n",
        "        df_pivot = df_work.pivot_table(index='Date', columns='Clean_Key', values=sess_col, aggfunc='sum').fillna(0)\n",
        "\n",
        "        # Apply Lag (Assume avg 1 day for report if not specified, or use optimized)\n",
        "        # For simplicity in report, we use a fixed lag of 1 or 0 unless passed\n",
        "        lag = 1\n",
        "        df_X = df_pivot.shift(lag).dropna()\n",
        "\n",
        "        # Build Stack\n",
        "        common_keys = [k for k in active_keys if k in df_X.columns]\n",
        "        if not common_keys: return None\n",
        "\n",
        "        df_plot = pd.DataFrame(index=df_X.index)\n",
        "        stack_cols = []\n",
        "\n",
        "        # Sort by impact\n",
        "        sorted_keys = sorted(common_keys, key=lambda k: model_weights[k] * df_X[k].sum(), reverse=True)\n",
        "        top_keys = sorted_keys[:8] # Top 8 for clean chart\n",
        "\n",
        "        for key in top_keys:\n",
        "            original_name = name_map.get(key, key)\n",
        "            df_plot[original_name] = df_X[key] * model_weights[key]\n",
        "            stack_cols.append(original_name)\n",
        "\n",
        "        # Merge Actuals\n",
        "        df_y = df_target.groupby('Date')[[trigger_col]].sum().rename(columns={trigger_col: 'Actual'})\n",
        "        df_final = pd.merge(df_y, df_plot, left_index=True, right_index=True, how='inner')\n",
        "\n",
        "        # Smooth for prettiness\n",
        "        df_final = df_final.rolling(window=7, min_periods=1).mean()\n",
        "\n",
        "        # PLOT\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        colors = sns.color_palette(\"tab20\", len(stack_cols))\n",
        "        plt.stackplot(df_final.index, df_final[stack_cols].T, labels=stack_cols, colors=colors, alpha=0.85)\n",
        "        plt.plot(df_final.index, df_final['Actual'], color='black', linewidth=2, label='Reported EVCs')\n",
        "\n",
        "        plt.title(f\"{title_prefix}: Traffic Source Breakdown\", fontsize=14, fontweight='bold')\n",
        "        plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Save to buffer\n",
        "        buf = io.BytesIO()\n",
        "        plt.savefig(buf, format='png', bbox_inches='tight')\n",
        "        plt.close()\n",
        "        return base64.b64encode(buf.getvalue()).decode('utf-8')\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error plotting {title_prefix}: {e}\")\n",
        "        return None\n",
        "\n",
        "def generate_full_report():\n",
        "    # --- 1. GATHER DATA ---\n",
        "    report_html = \"\"\n",
        "\n",
        "    # --- SECTION A: NNLS MODEL ---\n",
        "    # We look for 'current_results_df' (usually NNLS) or check if you saved it specifically\n",
        "    # If you only have one variable, we'll use it.\n",
        "    if 'current_results_df' in globals():\n",
        "        df_nnls = current_results_df\n",
        "\n",
        "        # Generate Chart\n",
        "        img_nnls = plot_to_base64(df_nnls, df_sessions, df_evc, \"NNLS Model (The 'Kill Switch')\")\n",
        "\n",
        "        report_html += f\"\"\"\n",
        "        <div class=\"metric-box\">\n",
        "            <h2>1. NNLS Model (Primary Attribution)</h2>\n",
        "            <p>This model identifies the <strong>primary drivers</strong> by forcing channels to compete for credit.</p>\n",
        "            {'<img src=\"data:image/png;base64,' + img_nnls + '\" style=\"width:100%; max-width:800px;\"/>' if img_nnls else '<p><em>Chart unavailable (check data).</em></p>'}\n",
        "            <br>\n",
        "            {df_nnls.head(8).to_html(classes='table', index=False)}\n",
        "        </div>\n",
        "        <hr>\n",
        "        \"\"\"\n",
        "\n",
        "    # --- SECTION B: RIDGE MODEL ---\n",
        "    # Check if a Ridge model exists (users often save it to 'ridge_results_df' or overwrite 'current')\n",
        "    if 'ridge_results_df' in globals() and ridge_results_df is not None:\n",
        "        df_ridge = ridge_results_df\n",
        "\n",
        "        # Generate Chart\n",
        "        img_ridge = plot_to_base64(df_ridge, df_sessions, df_evc, \"Ridge Model (The 'Halo Effect')\")\n",
        "\n",
        "        report_html += f\"\"\"\n",
        "        <div class=\"metric-box\">\n",
        "            <h2>2. Ridge Regression (Multi-Touch View)</h2>\n",
        "            <p>This model reveals the <strong>shared lift</strong> (\"Halo Effect\") where video ads influence multiple channels simultaneously.</p>\n",
        "            {'<img src=\"data:image/png;base64,' + img_ridge + '\" style=\"width:100%; max-width:800px;\"/>' if img_ridge else '<p><em>Chart unavailable.</em></p>'}\n",
        "            <br>\n",
        "            {df_ridge.head(8).to_html(classes='table', index=False)}\n",
        "        </div>\n",
        "        \"\"\"\n",
        "    elif 'current_results_df' in globals():\n",
        "        # Fallback if user overwrote the variable\n",
        "        report_html += \"<p><em>Note: Run Step 2.5 (Ridge) to see the comparative model here.</em></p>\"\n",
        "\n",
        "    # --- 3. BUILD FINAL HTML ---\n",
        "    full_html = f\"\"\"\n",
        "    <!DOCTYPE html>\n",
        "    <html>\n",
        "    <head>\n",
        "        <title>Video Attribution Report</title>\n",
        "        <style>\n",
        "            body {{ font-family: 'Helvetica', sans-serif; margin: 40px; color: #333; max-width: 1000px; margin: auto; }}\n",
        "            h1 {{ color: #1a73e8; border-bottom: 3px solid #1a73e8; padding-bottom: 10px; }}\n",
        "            h2 {{ color: #202124; margin-top: 30px; }}\n",
        "            .metric-box {{ background: #fff; padding: 20px; border: 1px solid #ddd; border-radius: 8px; box-shadow: 0 2px 5px rgba(0,0,0,0.05); margin-bottom: 30px; }}\n",
        "            .table {{ width: 100%; border-collapse: collapse; margin-top: 15px; font-size: 0.9em; }}\n",
        "            .table th {{ background: #f1f3f4; text-align: left; padding: 12px; border-bottom: 2px solid #ddd; }}\n",
        "            .table td {{ border-bottom: 1px solid #eee; padding: 10px; }}\n",
        "            .footer {{ margin-top: 50px; font-size: 0.8em; color: #777; text-align: center; }}\n",
        "        </style>\n",
        "    </head>\n",
        "    <body>\n",
        "        <h1>üé• Attribution & Ghost Traffic Analysis</h1>\n",
        "        <p><strong>Date:</strong> {pd.Timestamp.now().strftime('%Y-%m-%d')}</p>\n",
        "        <p>This report quantifies the incremental impact of Video campaigns on other traffic channels (Organic, Direct, Search).</p>\n",
        "\n",
        "        {report_html}\n",
        "\n",
        "        <div class=\"metric-box\">\n",
        "            <h2>3. Strategic Recommendations</h2>\n",
        "            <ul>\n",
        "                <li><strong>Validation:</strong> If the <em>Ridge Model</em> shows broader lift than NNLS, assume video is driving a general brand awareness lift rather than just specific channel clicks.</li>\n",
        "                <li><strong>Budgeting:</strong> Use the \"Attributed EVCs\" from the NNLS model to calculate your baseline ROAS.</li>\n",
        "            </ul>\n",
        "        </div>\n",
        "\n",
        "        <div class=\"footer\">Generated by the Ghost Traffic Attribution Tool</div>\n",
        "    </body>\n",
        "    </html>\n",
        "    \"\"\"\n",
        "\n",
        "    # --- 4. DOWNLOAD BUTTON ---\n",
        "    b64 = base64.b64encode(full_html.encode()).decode()\n",
        "    href = f'<a href=\"data:text/html;base64,{b64}\" download=\"Attribution_Master_Report.html\" target=\"_blank\">'\n",
        "    href += '<button style=\"background-color: #1a73e8; color: white; padding: 12px 24px; border: none; border-radius: 4px; font-size: 16px; cursor: pointer; font-weight: bold;\">üì• Download Full Report</button>'\n",
        "    href += '</a>'\n",
        "\n",
        "    return HTML(href)\n",
        "\n",
        "# Create Widget Wrapper\n",
        "btn_gen = widgets.Button(description=\"Generate Master Report\", button_style='info', icon='file-text', layout=widgets.Layout(width='250px'))\n",
        "out_gen = widgets.Output()\n",
        "\n",
        "display(widgets.VBox([\n",
        "    widgets.HTML(\"<h3>üìë Final Step: Download Master Report</h3><p>Generates a complete HTML dashboard with charts for both NNLS and Ridge models.</p>\"),\n",
        "    btn_gen, out_gen\n",
        "]))\n",
        "\n",
        "def on_gen_click(b):\n",
        "    with out_gen:\n",
        "        clear_output()\n",
        "        print(\"‚öôÔ∏è Generating charts and tables... (This may take a moment)\")\n",
        "        display(generate_full_report())\n",
        "\n",
        "btn_gen.on_click(on_gen_click)"
      ],
      "metadata": {
        "id": "OY_HIf5RvZue"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}